{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, Dense, Concatenate, TimeDistributed, LSTM, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from helper_funcs import gen_samples\n",
    "from scipy.fft import rfftfreq\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First navigate to our directory\n",
    "transfer_directory_path = os.path.join(\"Data\", \"synth_transfer_df.parquet\")\n",
    "general_directory_path = os.path.join(\"Data\", \"synth_general_df.parquet\")\n",
    "# Load the dataframes\n",
    "synth_transfer_df = pd.read_parquet(transfer_directory_path)\n",
    "synth_general_df = pd.read_parquet(general_directory_path)\n",
    "# Concatenate (after making sure they share columns) and then reset indices\n",
    "assert list(synth_transfer_df.columns) == list(synth_general_df.columns), \"Column names do not match!\"\n",
    "df = pd.concat([synth_transfer_df, synth_general_df], axis=0)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train (70%) and temp (30%) with stratification\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.3,\n",
    "    stratify=df['species'],  # Stratify based on the 'species' column\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split temp into test (15%) and validation (15%)\n",
    "test_df, val_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df['species'],  # Stratify again to maintain balance\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Training Samples\n"
     ]
    }
   ],
   "source": [
    "# Prepare samples\n",
    "print(\"Generating Training Samples\")\n",
    "X_train, y_train, mins_maxes_train, isolated_peaks_train = gen_samples(train_df)\n",
    "print(\"Generating Test Samples\")\n",
    "X_test, y_test, mins_maxes_test, isolated_peaks_test = gen_samples(test_df)\n",
    "print(\"Generating Validation Samples\")\n",
    "X_val, y_val, mins_maxes_val, isolated_peaks_val = gen_samples(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train[:, :, 0]\n",
    "y_test = y_test[:, :, 0]\n",
    "y_val = y_val[:, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def weight_func(prom, k=3, peak_encourage=10):\n",
    "    return peak_encourage*((prom/k)**k) / (1 + (prom/k)**k)\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function for (batch_size, N):\n",
    "    - Binary cross-entropy for the output node.\n",
    "    - Each bin in each sample is weighted by f(SNR), where SNR is the 3rd node label.\n",
    "    \n",
    "    Args:\n",
    "    y_true: Tensor of true labels, shape (batch_size, N, 3).\n",
    "    y_pred: Tensor of predicted values, shape (batch_size, N, 3).\n",
    "    \n",
    "    Returns:\n",
    "    A scalar tensor representing the combined loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # Check for any negative values or values greater than 1 in y_true\n",
    "    has_negative_y_true = tf.reduce_any(y_true < 0)\n",
    "    has_greater_one_y_true = tf.reduce_any(y_true > 1)\n",
    "    \n",
    "    # Check for any negative values in y_pred\n",
    "    has_negative_y_pred = tf.reduce_any(y_pred < 0)\n",
    "    \n",
    "    # Use tf.debugging.assert_* to enforce conditions during graph execution\n",
    "    tf.debugging.assert_equal(\n",
    "        has_negative_y_true, False, message=\"y_true contains negative values.\"\n",
    "    )\n",
    "    tf.debugging.assert_equal(\n",
    "        has_greater_one_y_true, False, message=\"y_true contains values greater than 1.\"\n",
    "    )\n",
    "    tf.debugging.assert_equal(\n",
    "        has_negative_y_pred, False, message=\"y_pred contains negative values.\"\n",
    "    )\n",
    "    # # Manually calculate binary cross-entropy for the first node\n",
    "    # epsilon = 1e-7  # Small constant to prevent log(0)\n",
    "    # y_pred_clipped = tf.clip_by_value(y_pred[..., 0], epsilon, 1.0 - epsilon)\n",
    "    # bce_loss = -(y_true[..., 0] * tf.math.log(y_pred_clipped) + (1 - y_true[..., 0]) * tf.math.log(1 - y_pred_clipped))  # Shape (batch_size, N)\n",
    "\n",
    "    # # Weighting each bin by weight_func(prom), where prominence is the 3rd node label\n",
    "    # prom = y_true[..., 2]  # prom is the 3rd node label, shape (batch_size, N)\n",
    "    # weights = tf.where(prom < 0, tf.ones_like(prom), weight_func(snr))  # If prom < 0, weight is 1 (fully weight the BCE loss for non-peak bins), else apply weight_func\n",
    "    \n",
    "    # Use Keras's built-in binary cross-entropy loss\n",
    "    bce = BinaryCrossentropy(reduction='none')  # No reduction so we can weight the loss per sample\n",
    "    \n",
    "    # Compute binary cross-entropy loss for the first node\n",
    "    bce_loss = bce(y_true[..., 0], y_pred[..., 0])  # Shape (batch_size, N)\n",
    "\n",
    "    # Weighting each bin by weight_func(prom), where prominence is the 3rd node label\n",
    "    prom = y_true[..., 2]  # prom is the 3rd node label, shape (batch_size, N)\n",
    "\n",
    "    weights = tf.where(prom < 0, tf.ones_like(prom), weight_func(prom))  # If prom < 0, weight is 1 (fully weight the BCE loss for non-peak bins), else apply weight_func\n",
    "    \n",
    "    # Apply weights to the BCE loss\n",
    "    weighted_bce_loss = bce_loss * weights  # Shape (batch_size, N)\n",
    "\n",
    "\n",
    "    # Sum BCE losses across bins (N) for each sample\n",
    "    mean_bce_loss_per_sample = tf.reduce_sum(weighted_bce_loss, axis=1, keepdims=False)\n",
    "\n",
    "    # Combine and average across the batch\n",
    "    total_loss = tf.reduce_mean(mean_bce_loss_per_sample)  # Mean over batch size\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "\n",
    "def peak_counting_error(isolated_peaks_val, val_predictions):\n",
    "    M = isolated_peaks_val.shape[0] # Number of samples\n",
    "    assert M == val_predictions.shape[0], \"Mismatch in number of samples!\"\n",
    "    best_error = 10000  # Initial large value for minimizing error\n",
    "    best_thresh = None\n",
    "    for thresh in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "        current_error = 0\n",
    "        val_predictions_snapped = (val_predictions > thresh).astype(int)\n",
    "        \n",
    "        for row in range(M):\n",
    "            predictions_row = val_predictions_snapped[row, :]\n",
    "            isolated_labels_row = isolated_peaks_val[row, :]\n",
    "            \n",
    "            # We want to go along the predictions row, and find continuous chunks of 1s and 0s.\n",
    "            # Every time a chunk ends, we then check how many peaks were truly in that chunk (by counting the 1s in those indices in isolated_labels_row)\n",
    "            # We then add the square of the differences between the predicted number of peaks and the actual number of peaks to total_error\n",
    "            # The predicted number of peaks in a chunk of 1s is always 1, and the predicted number of peaks in a chunk of 0s is always 0.\n",
    "            \n",
    "            # Track the current chunk\n",
    "            current_chunk_value = predictions_row[0]\n",
    "            current_chunk_start = 0\n",
    "\n",
    "            for idx in range(1, len(predictions_row) + 1):  # +1 to handle the last chunk\n",
    "                if idx == len(predictions_row) or predictions_row[idx] != current_chunk_value:\n",
    "                    # Chunk ends here\n",
    "                    chunk_end = idx\n",
    "                    chunk_labels = isolated_labels_row[current_chunk_start:chunk_end]\n",
    "                    \n",
    "                    # Predicted peaks for this chunk\n",
    "                    predicted_peaks = current_chunk_value\n",
    "                    # Actual peaks for this chunk\n",
    "                    actual_peaks = int(chunk_labels.sum())  # Count the 1s in the chunk\n",
    "                    \n",
    "                    # Add squared error to total_error\n",
    "                    current_error += (predicted_peaks - actual_peaks) ** 2\n",
    "                    \n",
    "                    # Start a new chunk\n",
    "                    current_chunk_value = predictions_row[idx] if idx < len(predictions_row) else None\n",
    "                    current_chunk_start = idx\n",
    "        \n",
    "        current_error = current_error / M\n",
    "        print(f\"Peak counting error for threshold {thresh}: {current_error}\")\n",
    "        # Update the best threshold if this one performs better\n",
    "        if current_error < best_error:\n",
    "            best_error = current_error\n",
    "            best_thresh = thresh\n",
    "    \n",
    "    print(f\"Best threshold: {best_thresh}, Best Peak Counting Error: {best_error}\")\n",
    "    return best_error, best_thresh\n",
    "        \n",
    "class ValidationMetricCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data, metric_name=\"peak_counting_error\"):\n",
    "        super(ValidationMetricCallback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.metric_name = metric_name\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_x, (val_y, isolated_peaks_val) = self.validation_data  # Unpack extra labels\n",
    "        val_predictions = self.model.predict(val_x, verbose=0)\n",
    "        \n",
    "        M = isolated_peaks_val.shape[0] # Number of samples\n",
    "        assert M == val_predictions.shape[0], \"Mismatch in number of samples!\"\n",
    "        best_error = 10000  # Initial large value for minimizing error\n",
    "        best_thresh = None\n",
    "        for thresh in [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "            current_error = 0\n",
    "            val_predictions_snapped = (val_predictions > thresh).astype(int)\n",
    "            \n",
    "            for row in range(M):\n",
    "                predictions_row = val_predictions_snapped[row, :]\n",
    "                isolated_labels_row = isolated_peaks_val[row, :]\n",
    "                \n",
    "                # We want to go along the predictions row, and find continuous chunks of 1s and 0s.\n",
    "                # Every time a chunk ends, we then check how many peaks were truly in that chunk (by counting the 1s in those indices in isolated_labels_row)\n",
    "                # We then add the square of the differences between the predicted number of peaks and the actual number of peaks to total_error\n",
    "                # The predicted number of peaks in a chunk of 1s is always 1, and the predicted number of peaks in a chunk of 0s is always 0.\n",
    "                \n",
    "                # Track the current chunk\n",
    "                current_chunk_value = predictions_row[0]\n",
    "                current_chunk_start = 0\n",
    "\n",
    "                for idx in range(1, len(predictions_row) + 1):  # +1 to handle the last chunk\n",
    "                    if idx == len(predictions_row) or predictions_row[idx] != current_chunk_value:\n",
    "                        # Chunk ends here\n",
    "                        chunk_end = idx\n",
    "                        chunk_labels = isolated_labels_row[current_chunk_start:chunk_end]\n",
    "                        \n",
    "                        # Predicted peaks for this chunk\n",
    "                        predicted_peaks = current_chunk_value\n",
    "                        # Actual peaks for this chunk\n",
    "                        actual_peaks = int(chunk_labels.sum())  # Count the 1s in the chunk\n",
    "                        \n",
    "                        # Add squared error to total_error\n",
    "                        current_error += (predicted_peaks - actual_peaks) ** 2\n",
    "                        \n",
    "                        # Start a new chunk\n",
    "                        current_chunk_value = predictions_row[idx] if idx < len(predictions_row) else None\n",
    "                        current_chunk_start = idx\n",
    "            \n",
    "            current_error = current_error / M\n",
    "            print(f\"Peak counting error for threshold {thresh}: {current_error}\")\n",
    "            # Update the best threshold if this one performs better\n",
    "            if current_error < best_error:\n",
    "                best_error = current_error\n",
    "                best_thresh = thresh\n",
    "        \n",
    "        print(f\"Best threshold: {best_thresh}, Best Peak Counting Error: {best_error}\")\n",
    "        peak_counting_error = best_error\n",
    "        # Add the validation metric to logs\n",
    "        logs[self.metric_name] = peak_counting_error.numpy()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: {self.metric_name} = {peak_counting_error.numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define name for this model\n",
    "model_version = \"V1\"\n",
    "\n",
    "# Define the input length / number of frequency bins (N)\n",
    "N = 8192\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(N, 1), name=\"Input\")\n",
    "\n",
    "# Inception-like layer with 1D convolutions\n",
    "convs = []\n",
    "# We'll base our kernel choices on the hwhm distribution of the peaks. \n",
    "# Thin peaks are in 3Hz-10Hz range --> 5-15 bins\n",
    "# Wide peaks are in 10Hz-100Hz range --> 15-149 bins\n",
    "# We choose filters at a range of scales, odd (to facilitate being cenetered around a peak)\n",
    "# and we want more filters for the medium-small range since there are more peaks at this scale.\n",
    "# Otherwise largely arbitrarily.\n",
    "kernels = [(3, 4), (5, 8), (9, 16), (15, 32), (31, 32), (55, 32), (71, 16), (101, 8), (149, 4), (201, 2)]\n",
    "for kernel_size, num_filters in kernels:\n",
    "    convs.append(Conv1D(num_filters, kernel_size=kernel_size, activation='relu', padding='same', name=f\"Conv_{kernel_size}\")(input_layer))\n",
    "\n",
    "# Concatenate the outputs of all convolutional layers\n",
    "concat_layer = Concatenate(name=\"Inception_Concat\")(convs)\n",
    "\n",
    "# Time Distributed Dense Layers\n",
    "td_dense64 = TimeDistributed(Dense(64, activation='relu'), name=\"Dense_64\")(concat_layer)\n",
    "td_dense32A = TimeDistributed(Dense(32, activation='relu'), name=\"Dense_32A\")(td_dense64)\n",
    "# bd_LSTM = Bidirectional(LSTM(16, return_sequences=True), name=\"LSTM\")(td_dense32A)\n",
    "# td_dense32B = TimeDistributed(Dense(32, activation='relu'), name=\"Dense_32B\")(bd_LSTM)\n",
    "td_dense32B = TimeDistributed(Dense(32, activation='relu'), name=\"Dense_32B\")(td_dense32A)\n",
    "td_dense16 = TimeDistributed(Dense(16, activation='relu'), name=\"Dense_16\")(td_dense32B)\n",
    "\n",
    "# Final layer with 1 output per input bin\n",
    "output = TimeDistributed(Dense(1, activation='sigmoid'), name=\"Output\")(td_dense16)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=input_layer, outputs=output, name=model_version)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='binary_crossentropy')\n",
    "\n",
    "# Summary\n",
    "# model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "169/169 [==============================] - 186s 1s/step - loss: 0.3425 - val_loss: 0.1133\n",
      "Epoch 2/5\n",
      "169/169 [==============================] - 185s 1s/step - loss: 0.1116 - val_loss: 0.1126\n",
      "Epoch 3/5\n",
      "169/169 [==============================] - 190s 1s/step - loss: 0.1115 - val_loss: 0.1126\n",
      "Epoch 4/5\n",
      "169/169 [==============================] - 188s 1s/step - loss: 0.1115 - val_loss: 0.1127\n",
      "Epoch 5/5\n",
      "169/169 [==============================] - 189s 1s/step - loss: 0.1115 - val_loss: 0.1126\n"
     ]
    }
   ],
   "source": [
    "# Define batch size, number of epochs, and patience\n",
    "batch_size = 32\n",
    "epochs = 5\n",
    "patience = 3\n",
    "\n",
    "model_path = os.path.join(\"PP Model\", f\"{model_version}.keras\")\n",
    "\n",
    "\n",
    "\n",
    "# Add callbacks for better training\n",
    "callbacks = [\n",
    "    ValidationMetricCallback(validation_data=(X_val, (y_val, isolated_peaks_val)), metric_name=\"peak_counting_error\"),\n",
    "    EarlyStopping(monitor=\"peak_counting_error\", patience=patience, restore_best_weights=True, verbose=1),  # Stop if no improvement for 5 epochs\n",
    "    ModelCheckpoint(model_path, save_best_only=True, monitor='peak_counting_error')  # Save the best model\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train,                # Training data\n",
    "    y_train,                # Training labels\n",
    "    validation_data=(X_val, y_val),  # Validation data\n",
    "    epochs=epochs,        # Number of epochs\n",
    "    batch_size=batch_size,  # Batch size\n",
    "    callbacks=callbacks,    # Add callbacks for early stopping and checkpointing\n",
    "    verbose=1               # Verbose output\n",
    ")\n",
    "\n",
    "with open(f'{model_version}_history.pkl', 'wb') as file:\n",
    "    pickle.dump(history.history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_version = \"V1\"\n",
    "model = load_model(os.path.join(\"PP Model\", f\"{model_version}.keras\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37/37 [==============================] - 5s 137ms/step\n"
     ]
    }
   ],
   "source": [
    "# Assuming the predicted values are probabilities from 0 to 1\n",
    "val_pred = model.predict(X_val)  # Predicted probabilities or binary values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peak counting error for threshold 0.1: [437.33391154]\n",
      "Peak counting error for threshold 0.2: [437.33391154]\n",
      "Peak counting error for threshold 0.3: [437.33391154]\n"
     ]
    }
   ],
   "source": [
    "best_error, best_thresh = peak_counting_error(isolated_peaks_val, val_pred)\n",
    "sample_idx = 0\n",
    "thresh = best_thresh\n",
    "spectrum = X_val[sample_idx, :]\n",
    "predicted_peaks = val_pred[sample_idx, :]  # Assuming it's binary (0s and 1s)\n",
    "\n",
    "# Frequency axis for the spectrum\n",
    "f = rfftfreq(32768, 1/44100)[0:8192]\n",
    "\n",
    "# Find indices where there are peaks (1 in predicted_peaks)\n",
    "peak_indices = np.where(predicted_peaks > thresh)[0]\n",
    "\n",
    "# Plot the spectrum\n",
    "plt.plot(f, spectrum, label='Original Spectrum')\n",
    "\n",
    "# Plot the peaks as scatter points\n",
    "plt.scatter(f[peak_indices], spectrum[peak_indices], color='red', label='Predicted Peaks')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get validation counting accuracy\n",
    "val_peak_counting_accuracy = peak_counting_accuracy(isolated_peaks_val, val_pred)\n",
    "\n",
    "print(\"Test Loss:\", test_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Custom Loss\n",
    "# Example data: batch_size=4, N=5, nodes=3\n",
    "y_true = np.array([\n",
    "    [[0, 0.5, 0.7], [0, 0.2, -1], [0, 10000, -1000], [0, 0.3, 10], [0, 0.1, 10]],  # Sample 1\n",
    "    [[0, 0.6, 0.3], [1, 0.1, -1], [1, 0.3, 10], [0, 0.4, 10], [0, 0.7, 10]],  # Sample 2\n",
    "    [[0, 0.4, 1.5], [1, 0.8, -1], [1, 0.6, 10], [1, 0.2, 10], [0, 0.9, 10]],  # Sample 3\n",
    "    [[0, 0.5, 0.6], [0, 0.3, -1], [0, 0.7, 10], [1, 0.1, 10], [0, 0.8, 10]],  # Sample 4\n",
    "])\n",
    "\n",
    "y_pred = np.array([\n",
    "    [[0.9, 0.6, 0.8], [0.9, 0.3, 0.5], [0.5, 100000, 1000], [0.7, 0.4, 0.6], [0.2, 0.1, 0.3]],  # Sample 1\n",
    "    [[0.7, 0.5, 0.4], [0.9, 0.2, 0.3], [0.9, 0.4, 0.6], [0.6, 0.7, 0.9], [0.8, 0.7, 0.8]],  # Sample 2\n",
    "    [[0.8, 0.4, 0.5], [0.9, 0.6, 0.8], [0.9, 0.7, 0.5], [0.9, 0.3, 0.6], [0.2, 0.9, 0.7]],  # Sample 3\n",
    "    [[0.9, 0.4, 0.3], [0.9, 0.6, 0.8], [0.6, 0.9, 0.7], [0.9, 0.3, 0.5], [0.8, 0.7, 0.6]],  # Sample 4\n",
    "])\n",
    "\n",
    "# Convert to tensors\n",
    "y_true_tensor = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "y_pred_tensor = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "\n",
    "loss_value = custom_loss(y_true_tensor, y_pred_tensor)\n",
    "print(\"Loss Value:\", loss_value.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot a sample\n",
    "i = 1\n",
    "spectrum = X_train[i]\n",
    "f = rfftfreq(32768, 1/44100)[0:8192]\n",
    "\n",
    "labels = y_train[i, :, :]\n",
    "spread_peak_labels = labels[:, 0]\n",
    "indices = np.where(spread_peak_labels == 1)[0]\n",
    "# isolated_peaks = isolated_peaks_train[i]\n",
    "# indices = np.where(isolated_peaks == 1)[0]\n",
    "plt.scatter(f, spectrum, color='blue', s=1)\n",
    "plt.scatter(f[indices], spectrum[indices], color='red', s=1)\n",
    "plt.xlim(3000, 4000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify peak_counting_error\n",
    "isolated_peaks_val=np.array(\n",
    "    [[0, 1, 0, 0, 1], \n",
    "     [1, 0, 1, 0, 0]]\n",
    "    )\n",
    "val_predictions=np.array(\n",
    "    [[0.81, 0.91, 0.71, 0.31, 0.91],\n",
    "     [0.81, 0.21, 0.91, 0.96, 0.91]]\n",
    "    )\n",
    "\n",
    "peak_counting_error(isolated_peaks_val, val_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out how many bins our peaks are\n",
    "f = rfftfreq(32768, 1/44100)\n",
    "# HWFM (in bins) of a peak with a HWHM of 100Hz\n",
    "bin_width = f[1] - f[0]\n",
    "print(f\"HWFM (in bins) of a peak with a HWHM of 100Hz: {200 / bin_width}\")\n",
    "print(f\"HWFM (in bins) of a peak with a HWHM of 10Hz: {20 / bin_width}\")\n",
    "print(f\"HWFM (in bins) of a peak with a HWHM of 3Hz: {6 / bin_width}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def custom_loss(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Custom loss function for (batch_size, N, 3):\n",
    "#     - Binary cross-entropy for the first output node.\n",
    "#     - MSE for the second and third output nodes, masked by the first node's true labels.\n",
    "#     - Each bin in each sample is weighted by f(SNR), where SNR is the 3rd node label.\n",
    "    \n",
    "#     Args:\n",
    "#     y_true: Tensor of true labels, shape (batch_size, N, 3).\n",
    "#     y_pred: Tensor of predicted values, shape (batch_size, N, 3).\n",
    "    \n",
    "#     Returns:\n",
    "#     A scalar tensor representing the combined loss.\n",
    "#     \"\"\"\n",
    "\n",
    "    # # Mean squared error for the second and third nodes\n",
    "    # mse_loss_2 = tf.square(y_true[..., 1] - y_pred[..., 1])\n",
    "    # mse_loss_3 = tf.square(y_true[..., 2] - y_pred[..., 2])\n",
    "    # mse_loss = mse_loss_2 + mse_loss_3  # Shape (batch_size, N)\n",
    "\n",
    "    # # Mask the MSE loss where the first node's true label is 0\n",
    "    # mask = tf.cast(y_true[..., 0] > 0, tf.float32)  # Shape (batch_size, N)\n",
    "    # masked_mse_loss = mse_loss * mask  # Shape (batch_size, N)\n",
    "    \n",
    "    # # Manually calculate binary cross-entropy for the first node\n",
    "    # epsilon = 1e-7  # Small constant to prevent log(0)\n",
    "    # y_pred_clipped = tf.clip_by_value(y_pred[..., 0], epsilon, 1.0 - epsilon)\n",
    "    # bce_loss = -(y_true[..., 0] * tf.math.log(y_pred_clipped) + (1 - y_true[..., 0]) * tf.math.log(1 - y_pred_clipped))  # Shape (batch_size, N)\n",
    "\n",
    "    # # Weighting each bin by weight_func(SNR), where SNR is the 3rd node label\n",
    "    # snr = y_true[..., 2]  # SNR is the 3rd node label, shape (batch_size, N)\n",
    "    # weights = tf.where(snr < 0, tf.ones_like(snr), weight_func(snr))  # If SNR < 0, weight is 1 (fully weight the BCE loss for non-peak bins), else apply weight_func\n",
    "\n",
    "    # # Apply weights to the masked MSE loss\n",
    "    # weighted_mse_loss = masked_mse_loss * weights  # Shape (batch_size, N)\n",
    "    \n",
    "    # # Apply weights to the BCE loss\n",
    "    # weighted_bce_loss = bce_loss * weights  # Shape (batch_size, N)\n",
    "\n",
    "    # # Average weighted MSE, BCE losses across bins (N) for each sample\n",
    "    # mean_mse_loss_per_sample = tf.reduce_mean(weighted_mse_loss, axis=1)  # Mean over N for shape (batch_size,)\n",
    "    # mean_bce_loss_per_sample = tf.reduce_mean(weighted_bce_loss, axis=1)\n",
    "\n",
    "    # # Combine and average across the batch\n",
    "    # total_loss = tf.reduce_mean(mean_bce_loss_per_sample + mean_mse_loss_per_sample)  # Mean over batch size\n",
    "\n",
    "    # return total_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class ValidationMetricCallback(tf.keras.callbacks.Callback):\n",
    "#     def __init__(self, validation_data, metric_name=\"peak_counting_error\"):\n",
    "#         super(ValidationMetricCallback, self).__init__()\n",
    "#         self.validation_data = validation_data\n",
    "#         self.metric_name = metric_name\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs=None):\n",
    "#         val_x, (val_y, isolated_peaks_val) = self.validation_data  # Unpack extra labels\n",
    "#         val_predictions = self.model.predict(val_x, verbose=0)\n",
    "        \n",
    "#         # Compute your custom metric (e.g., Mean Absolute Error)\n",
    "#         val_metric = peak_counting_error(isolated_peaks_val, val_predictions)\n",
    "\n",
    "#         # Add the validation metric to logs\n",
    "#         logs[self.metric_name] = val_metric.numpy()\n",
    "\n",
    "#         print(f\"Epoch {epoch + 1}: {self.metric_name} = {val_metric.numpy()}\")\n",
    "\n",
    "\n",
    "# for row_idx in range(y_train.shape[0]):\n",
    "#     y_peak_labels = y_train[:, :, 0]\n",
    "#     if tf.reduce_any(y_peak_labels[row_idx] < 0):  # Check if any value in the row is negative\n",
    "#         print(f\"Row {row_idx} contains negative values:\")\n",
    "#         print(y_peak_labels[row_idx])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
