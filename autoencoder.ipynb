{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-21 17:15:32.493345: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-21 17:15:40.365287: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2024-11-21 17:15:40.365358: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2024-11-21 17:15:41.085008: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-11-21 17:15:56.652271: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-21 17:15:56.652706: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-11-21 17:15:56.652733: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from processing_funcs import *\n",
    "from tensorflow.python.keras.models import Model\n",
    "from tensorflow.python.keras.layers import Input, Conv1D, MaxPooling1D, UpSampling1D\n",
    "from tensorflow.python.keras.regularizers import l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading York Data 1\n",
      "Combining into one Dataframe!\n",
      "Processing wf 1/83\n",
      "Processing wf 2/83\n",
      "Processing wf 3/83\n",
      "Processing wf 4/83\n",
      "Processing wf 5/83\n",
      "Processing wf 6/83\n",
      "Processing wf 7/83\n",
      "Processing wf 8/83\n",
      "Processing wf 9/83\n",
      "Processing wf 10/83\n",
      "Processing wf 11/83\n",
      "Processing wf 12/83\n",
      "Processing wf 13/83\n",
      "Processing wf 14/83\n",
      "Processing wf 15/83\n",
      "Processing wf 16/83\n",
      "Processing wf 17/83\n",
      "Processing wf 18/83\n",
      "Processing wf 19/83\n",
      "Processing wf 20/83\n",
      "Processing wf 21/83\n",
      "Processing wf 22/83\n",
      "Processing wf 23/83\n",
      "Processing wf 24/83\n",
      "Processing wf 25/83\n",
      "Processing wf 26/83\n",
      "Processing wf 27/83\n",
      "Processing wf 28/83\n",
      "Processing wf 29/83\n",
      "Processing wf 30/83\n",
      "Processing wf 31/83\n",
      "Processing wf 32/83\n",
      "Processing wf 33/83\n",
      "Processing wf 34/83\n",
      "Processing wf 35/83\n",
      "Processing wf 36/83\n",
      "Processing wf 37/83\n",
      "Processing wf 38/83\n",
      "Processing wf 39/83\n",
      "Processing wf 40/83\n",
      "Processing wf 41/83\n",
      "Processing wf 42/83\n",
      "Processing wf 43/83\n",
      "Processing wf 44/83\n",
      "Processing wf 45/83\n",
      "Processing wf 46/83\n",
      "Processing wf 47/83\n",
      "Processing wf 48/83\n",
      "Processing wf 49/83\n",
      "Processing wf 50/83\n",
      "Processing wf 51/83\n",
      "Processing wf 52/83\n",
      "Processing wf 53/83\n",
      "Processing wf 54/83\n",
      "Processing wf 55/83\n",
      "Processing wf 56/83\n",
      "Processing wf 57/83\n",
      "Processing wf 58/83\n",
      "Processing wf 59/83\n",
      "Processing wf 60/83\n",
      "Processing wf 61/83\n",
      "Processing wf 62/83\n",
      "Processing wf 63/83\n",
      "Processing wf 64/83\n",
      "Processing wf 65/83\n",
      "Processing wf 66/83\n",
      "Processing wf 67/83\n",
      "Processing wf 68/83\n",
      "Processing wf 69/83\n",
      "Processing wf 70/83\n",
      "Processing wf 71/83\n",
      "Processing wf 72/83\n",
      "Processing wf 73/83\n",
      "Processing wf 74/83\n",
      "Processing wf 75/83\n",
      "Processing wf 76/83\n",
      "Processing wf 77/83\n",
      "Processing wf 78/83\n",
      "Processing wf 79/83\n",
      "Processing wf 80/83\n",
      "Processing wf 81/83\n",
      "Processing wf 82/83\n",
      "Processing wf 83/83\n"
     ]
    }
   ],
   "source": [
    "# Load dataframe and prepare samples\n",
    "df = load_df()\n",
    "df = process_wfs(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration variables\n",
    "HLactivation = 'relu'               # Activation for hidden layers\n",
    "outputactivation = 'sigmoid'        # Activation for output layer\n",
    "filtersize = 5                      # Variable kernel size for Conv1D layers\n",
    "use_l1_regularization = True        # Toggle L1 regularization on or off\n",
    "l1_lambda = 0.001                   # L1 regularization coefficient, if applicable\n",
    "\n",
    "# Loss weights\n",
    "weighted_loss_type = 'exponential'  # Options: 'exponential' or 'square'\n",
    "weight_min = 100                  # Minimal freq_bin index to weight. Avoids weighting the near zero \"peak\".\n",
    "\n",
    "# Channel numbers for each Conv1D layer\n",
    "channel_num1 = 32\n",
    "channel_num2 = 16\n",
    "channel_num3 = 8\n",
    "\n",
    "# Max pooling down/upsampling ratio\n",
    "MP_ratio = 4\n",
    "\n",
    "# Define a weighted loss function based on the variable choice\n",
    "def weighted_mse(y_true, y_pred):\n",
    "    if weighted_loss_type == 'exponential':\n",
    "        weights = tf.exp(y_true)\n",
    "    elif weighted_loss_type == 'square':\n",
    "        weights = tf.square(y_true)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid weighted_loss_type. Choose 'exponential' or 'square'.\")\n",
    "    # If we're below the cutoff, just replace the weight with the lowest calculated weight.\n",
    "    # This way we don't care much about the universal near zero \"peak\"\n",
    "    weights[0:weight_min] = np.min(weights)\n",
    "    return tf.reduce_mean(weights * tf.square(y_true - y_pred))\n",
    "\n",
    "# Define the encoder\n",
    "input_data = Input(shape=(1000, 1))\n",
    "\n",
    "# Optional L1 regularization\n",
    "if use_l1_regularization:\n",
    "    regularizer = l1(l1_lambda)\n",
    "else:\n",
    "    regularizer = None\n",
    "\n",
    "# Encoder with variable channel numbers\n",
    "x = Conv1D(channel_num1, filtersize, activation=HLactivation, padding='same', kernel_regularizer=regularizer)(input_data)\n",
    "x = MaxPooling1D(MP_ratio, padding='same')(x)\n",
    "x = Conv1D(channel_num2, filtersize, activation=HLactivation, padding='same', kernel_regularizer=regularizer)(x)\n",
    "x = MaxPooling1D(MP_ratio, padding='same')(x)\n",
    "x = Conv1D(channel_num3, filtersize, activation=HLactivation, padding='same', kernel_regularizer=regularizer)(x)\n",
    "encoded = MaxPooling1D(MP_ratio, padding='same')(x)\n",
    "\n",
    "# Decoder with variable channel numbers\n",
    "x = Conv1D(channel_num3, filtersize, activation=HLactivation, padding='same', kernel_regularizer=regularizer)(encoded)\n",
    "x = UpSampling1D(MP_ratio)(x)\n",
    "x = Conv1D(channel_num2, filtersize, activation=HLactivation, padding='same', kernel_regularizer=regularizer)(x)\n",
    "x = UpSampling1D(MP_ratio)(x)\n",
    "x = Conv1D(channel_num1, filtersize, activation=HLactivation, padding='same', kernel_regularizer=regularizer)(x)\n",
    "x = UpSampling1D(MP_ratio)(x)\n",
    "decoded = Conv1D(1, filtersize, activation=outputactivation, padding='same')(x)\n",
    "\n",
    "# Define the autoencoder model\n",
    "autoencoder = Model(input_data, decoded)\n",
    "\n",
    "# Compile the model using the weighted loss function\n",
    "autoencoder.compile(optimizer='adam', loss=weighted_mse)\n",
    "\n",
    "# Print a summary of the model to verify the architecture\n",
    "autoencoder.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter [/fs/ess/PAS2038/PHYSICS_5680_OSU/jupyter/]",
   "language": "python",
   "name": "venv_jupyter"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
