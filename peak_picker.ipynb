{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, Dense, Concatenate, TimeDistributed, LSTM, Bidirectional, Layer\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.autograph.experimental import do_not_convert\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from helper_funcs import gen_samples\n",
    "from scipy.fft import rfftfreq\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and split dataset\n",
    "\n",
    "# File paths\n",
    "transfer_directory_path = os.path.join(\"Data\", \"synth_transfer_df.parquet\")\n",
    "general_directory_path = os.path.join(\"Data\", \"synth_general_df.parquet\")\n",
    "\n",
    "# Load the dataframes\n",
    "synth_transfer_df = pd.read_parquet(transfer_directory_path)\n",
    "synth_general_df = pd.read_parquet(general_directory_path)\n",
    "\n",
    "# Ensure column names match\n",
    "assert list(synth_transfer_df.columns) == list(synth_general_df.columns), \"Column names do not match!\"\n",
    "\n",
    "# Add a dataset identifier for stratification\n",
    "synth_transfer_df['dataset'] = 'transfer'\n",
    "synth_general_df['dataset'] = 'general'\n",
    "\n",
    "# Combine the datasets\n",
    "df = pd.concat([synth_transfer_df, synth_general_df], axis=0)\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Random state for reproducibility\n",
    "rs = 1\n",
    "\n",
    "print(len(df))\n",
    "\n",
    "# Split each dataset (transfer and general) independently\n",
    "transfer_train, transfer_temp = train_test_split(\n",
    "    synth_transfer_df,\n",
    "    test_size=0.3,  # 30% of transfer dataset\n",
    "    stratify=synth_transfer_df['species'],  # Stratify based on species\n",
    "    random_state=rs\n",
    ")\n",
    "\n",
    "general_train, general_temp = train_test_split(\n",
    "    synth_general_df,\n",
    "    test_size=0.3,  # 30% of general dataset\n",
    "    stratify=synth_general_df['species'],  # Stratify based on species\n",
    "    random_state=rs\n",
    ")\n",
    "\n",
    "# Combine the training datasets from transfer and general\n",
    "train_df = pd.concat([transfer_train, general_train], axis=0).reset_index(drop=True)\n",
    "\n",
    "# Split temp datasets (transfer and general) into test and validation\n",
    "transfer_test, transfer_val = train_test_split(\n",
    "    transfer_temp,\n",
    "    test_size=0.5,  # Split evenly into test and validation\n",
    "    stratify=transfer_temp['species'],\n",
    "    random_state=rs + 1\n",
    ")\n",
    "\n",
    "general_test, general_val = train_test_split(\n",
    "    general_temp,\n",
    "    test_size=0.5,  # Split evenly into test and validation\n",
    "    stratify=general_temp['species'],\n",
    "    random_state=rs + 1\n",
    ")\n",
    "\n",
    "# Combine test and validation datasets from transfer and general\n",
    "test_df = pd.concat([transfer_test, general_test], axis=0).reset_index(drop=True)\n",
    "val_df = pd.concat([transfer_val, general_val], axis=0).reset_index(drop=True)\n",
    "\n",
    "# # Verify distribution of species and datasets\n",
    "# print(\"Training dataset:\")\n",
    "# print(train_df['species'].value_counts())\n",
    "# print(train_df['dataset'].value_counts())\n",
    "\n",
    "# print(\"\\nValidation dataset:\")\n",
    "# print(val_df['species'].value_counts())\n",
    "# print(val_df['dataset'].value_counts())\n",
    "\n",
    "# print(\"\\nTest dataset:\")\n",
    "# print(test_df['species'].value_counts())\n",
    "# print(test_df['dataset'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare samples\n",
    "print(\"Generating Training Samples\")\n",
    "X_train, y_train, mins_maxes_train, isolated_peaks_train = gen_samples(train_df)\n",
    "print(\"Generating Test Samples\")\n",
    "X_test, y_test, mins_maxes_test, isolated_peaks_test = gen_samples(test_df)\n",
    "print(\"Generating Validation Samples\")\n",
    "X_val, y_val, mins_maxes_val, isolated_peaks_val = gen_samples(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Crop num samples for testing\n",
    "# M = 1000\n",
    "# X_train = X_train[:M, :]\n",
    "# X_val = X_val[:M, :]\n",
    "# X_test = X_test[:M, :]\n",
    "\n",
    "# y_train = y_train[:M, :, :]\n",
    "# y_val = y_val[:M, :, :]\n",
    "# y_test = y_test[:M, :, :]\n",
    "\n",
    "# isolated_peaks_val = isolated_peaks_val[:M, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand axes for conv layers\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_val = np.expand_dims(X_val, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=3\n",
    "peak_encourage=1\n",
    "include_LSTM=False\n",
    "epochs = 1\n",
    "lr = 0.001\n",
    "model_version = f\"V1_k-{k}_PE-{peak_encourage}_LSTM-{include_LSTM}_Epochs-{epochs}_LR-{lr}\"\n",
    "batch_size = 32\n",
    "patience = 15\n",
    "threshold_list = [0.3, 0.4, 0.5, 0.6, 0.7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define custom loss and metric callbacks\n",
    "def custom_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function using predictions and weights.\n",
    "    \"\"\"\n",
    "    # Compute binary cross-entropy loss\n",
    "    bce_loss = tf.keras.backend.binary_crossentropy(y_true[:, :, 0], y_pred[:, :, 0])  # Shape (batch_size, N)\n",
    "\n",
    "    # prom = y_true[:, :, 2] is the 3rd node label, shape (batch_size, N)\n",
    "    # If prom < 0, weight is 1 (weight of BCE loss for non-peak bins), else apply weight_func\n",
    "    weights = tf.where(y_true[:, :, 2] < 0, tf.ones_like(y_true[:, :, 2]), peak_encourage/(1+tf.exp(k-y_true[:, :, 2])))  \n",
    "    # Apply weights\n",
    "    weighted_bce_loss = bce_loss * weights  # Shape (batch_size, bins_per_sample)\n",
    "\n",
    "    # Average loss across all samples and bins\n",
    "    total_loss = tf.reduce_mean(weighted_bce_loss)  # Scalar\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def peak_counting_error(isolated_peaks, predictions, threshold_list, verbose=False):\n",
    "    # Just grab the labels since we're ignoring width and height\n",
    "    predictions = predictions[:, :, 0]\n",
    "    M = isolated_peaks.shape[0] # Number of samples\n",
    "    assert M == predictions.shape[0], \"Mismatch in number of samples!\"\n",
    "    best_error = 10000  # Initial large value for minimizing error\n",
    "    best_thresh = None\n",
    "    for thresh in threshold_list:\n",
    "        current_error = 0\n",
    "        val_predictions_snapped = (predictions > thresh).astype(int)\n",
    "        \n",
    "        for row in range(M):\n",
    "            predictions_row = val_predictions_snapped[row, :]\n",
    "            isolated_labels_row = isolated_peaks[row, :]\n",
    "            \n",
    "            # We want to go along the predictions row, and find continuous chunks of 1s and 0s.\n",
    "            # Every time a chunk ends, we then check how many peaks were truly in that chunk (by counting the 1s in those indices in isolated_labels_row)\n",
    "            # We then add the square of the differences between the predicted number of peaks and the actual number of peaks to total_error\n",
    "            # The predicted number of peaks in a chunk of 1s is always 1, and the predicted number of peaks in a chunk of 0s is always 0.\n",
    "            \n",
    "            # Track the current chunk\n",
    "            current_chunk_value = predictions_row[0]\n",
    "            current_chunk_start = 0\n",
    "\n",
    "            for idx in range(1, len(predictions_row) + 1):  # +1 to handle the last chunk\n",
    "                if idx == len(predictions_row) or predictions_row[idx] != current_chunk_value:\n",
    "                    # Chunk ends here\n",
    "                    chunk_end = idx\n",
    "                    chunk_labels = isolated_labels_row[current_chunk_start:chunk_end]\n",
    "                    \n",
    "                    # Predicted peaks for this chunk\n",
    "                    predicted_peaks = current_chunk_value\n",
    "                    # Actual peaks for this chunk\n",
    "                    actual_peaks = int(chunk_labels.sum())  # Count the 1s in the chunk\n",
    "                    \n",
    "                    # Add squared error to total_error\n",
    "                    current_error += (predicted_peaks - actual_peaks) ** 2\n",
    "                    \n",
    "                    # Start a new chunk\n",
    "                    current_chunk_value = predictions_row[idx] if idx < len(predictions_row) else None\n",
    "                    current_chunk_start = idx\n",
    "        \n",
    "        current_error = current_error / M\n",
    "        if verbose:\n",
    "            print(f\"Peak counting error for threshold {thresh}: {current_error}\")\n",
    "        # Update the best threshold if this one performs better\n",
    "        if current_error < best_error:\n",
    "            best_error = current_error\n",
    "            best_thresh = thresh\n",
    "    if verbose:\n",
    "        print(f\"Best threshold: {best_thresh}, Best Peak Counting Error: {best_error}\")\n",
    "    return best_error, best_thresh\n",
    "        \n",
    "\n",
    "class ValidationMetricCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data, metric_name=\"peak_counting_error\"):\n",
    "        super(ValidationMetricCallback, self).__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.metric_name = metric_name\n",
    "    \n",
    "    @do_not_convert\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_x, (val_y, isolated_peaks_val) = self.validation_data  # Unpack extra labels\n",
    "        val_predictions = self.model.predict(val_x, verbose=0)\n",
    "        val_predictions = val_predictions[:, :, 0]\n",
    "        M = tf.shape(isolated_peaks_val)[0]  # Number of samples\n",
    "        tf.assert_equal(M, tf.shape(val_predictions)[0], message=\"Mismatch in number of samples!\")\n",
    "        \n",
    "        thresholds = tf.constant(threshold_list, dtype=tf.float32)\n",
    "        best_error = tf.constant(1e10, dtype=tf.float32)  # Initial large value for minimizing error\n",
    "        best_thresh = tf.constant(0.0, dtype=tf.float32)\n",
    "\n",
    "        \n",
    "        def calculate_error(thresh):\n",
    "            val_predictions_snapped = tf.cast(val_predictions > thresh, tf.int32)\n",
    "\n",
    "            def process_row(row_idx):\n",
    "                predictions_row = val_predictions_snapped[row_idx]\n",
    "                isolated_labels_row = isolated_peaks_val[row_idx]\n",
    "\n",
    "                chunk_boundaries = tf.concat([[0], tf.where(tf.not_equal(predictions_row[:-1], predictions_row[1:]))[:, 0] + 1, [tf.size(predictions_row)]], axis=0)\n",
    "                chunk_start_indices = chunk_boundaries[:-1]\n",
    "                chunk_end_indices = chunk_boundaries[1:]\n",
    "\n",
    "                def process_chunk(start, end):\n",
    "                    chunk_labels = isolated_labels_row[start:end]\n",
    "                    predicted_peaks = tf.cast(predictions_row[start], tf.float32)  # Cast to float32\n",
    "                    actual_peaks = tf.cast(tf.reduce_sum(chunk_labels), tf.float32)  # Cast to float32\n",
    "                    return tf.square(predicted_peaks - actual_peaks)\n",
    "\n",
    "                squared_errors = tf.map_fn(\n",
    "                    lambda indices: process_chunk(indices[0], indices[1]),\n",
    "                    (chunk_start_indices, chunk_end_indices),\n",
    "                    fn_output_signature=tf.float32\n",
    "                )\n",
    "                return tf.reduce_sum(squared_errors)\n",
    "\n",
    "            total_error = tf.map_fn(process_row, tf.range(M), fn_output_signature=tf.float32)\n",
    "            return tf.reduce_mean(total_error)\n",
    "\n",
    "        def update_best(thresh, current_error, best_error, best_thresh):\n",
    "            better = current_error < best_error\n",
    "            return tf.cond(\n",
    "                better,\n",
    "                lambda: (current_error, thresh),\n",
    "                lambda: (best_error, best_thresh)\n",
    "            )\n",
    "\n",
    "        for thresh in thresholds:\n",
    "            current_error = calculate_error(thresh)\n",
    "            best_error, best_thresh = update_best(thresh, current_error, best_error, best_thresh)\n",
    "\n",
    "        # Round the best_thresh to 3 decimal places\n",
    "        rounded_best_thresh = tf.round(best_thresh * 1000) / 1000.0  # Rounds to 3 decimal places\n",
    "    \n",
    "        # Add space before printing the custom output\n",
    "        tf.print(\"\\n--------------------------------------------\")\n",
    "        tf.print(f\"Best threshold: {rounded_best_thresh}, Best Peak Counting Error: {best_error}\")\n",
    "        tf.print(\"--------------------------------------------\\n\")\n",
    "\n",
    "        logs[self.metric_name] = best_error\n",
    "\n",
    "\n",
    "class TimeHistory(tf.keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs=None):\n",
    "        self.epoch_times = []  # List to store time per epoch\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        self.epoch_start_time = time.time()  # Record start time of epoch\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        epoch_time = time.time() - self.epoch_start_time  # Calculate epoch duration\n",
    "        self.epoch_times.append(epoch_time)  # Save to list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the input length / number of frequency bins (N)\n",
    "N = 8192\n",
    "include_LSTM = False\n",
    "# Input layer\n",
    "input_layer = Input(shape=(N, 1), name=\"Input\")\n",
    "# Inception-like layer with 1D convolutions\n",
    "convs = []\n",
    "# We'll base our kernel choices on the hwhm distribution of the peaks. \n",
    "# Thin peaks are in 3Hz-10Hz range --> 5-15 bins\n",
    "# Wide peaks are in 10Hz-100Hz range --> 15-149 bins\n",
    "# We choose filters at a range of scales, odd (to facilitate being cenetered around a peak)\n",
    "# and we want more filters for the medium-small range since there are more peaks at this scale.\n",
    "# Otherwise largely arbitrarily.\n",
    "kernels = [(3, 4), (5, 8), (9, 16), (15, 32), (31, 32), (55, 32), (71, 16), (101, 8), (149, 4), (201, 2)]\n",
    "for kernel_size, num_filters in kernels:\n",
    "    convs.append(Conv1D(num_filters, kernel_size=kernel_size, activation='relu', padding='same', name=f\"Conv_{kernel_size}\")(input_layer))\n",
    "\n",
    "# Concatenate the outputs of all convolutional layers\n",
    "concat_layer = Concatenate(name=\"Inception_Concat\")(convs)\n",
    "\n",
    "# Time Distributed Dense Layers\n",
    "td_dense64 = TimeDistributed(Dense(64, activation='relu'), name=\"Dense_64\")(concat_layer)\n",
    "td_dense32A = TimeDistributed(Dense(32, activation='relu'), name=\"Dense_32A\")(td_dense64)\n",
    "if include_LSTM:\n",
    "    bd_LSTM = Bidirectional(LSTM(16, return_sequences=True), name=\"LSTM\")(td_dense32A)\n",
    "    td_dense32B = TimeDistributed(Dense(32, activation='relu'), name=\"Dense_32B\")(bd_LSTM)\n",
    "else:\n",
    "    td_dense32B = TimeDistributed(Dense(32, activation='relu'), name=\"Dense_32B\")(td_dense32A)\n",
    "td_dense16 = TimeDistributed(Dense(16, activation='relu'), name=\"Dense_16\")(td_dense32B)\n",
    "\n",
    "# Final layer with 3 outputs per input bin\n",
    "output_layer = TimeDistributed(Dense(3, activation='sigmoid'), name=\"Output\")(td_dense16)\n",
    "\n",
    "# Define the model to output both predictions and weights\n",
    "model = tf.keras.Model(\n",
    "    inputs=input_layer, \n",
    "    outputs=output_layer,  # Explicitly define both outputs\n",
    "    name=model_version\n",
    ")\n",
    "\n",
    "\n",
    "# Compile the model (lambda function in loss to allow for prominences to be passed in as weights)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "    # loss=lambda y_true, y_pred: custom_loss(y_true, y_pred, weights)\n",
    "    loss=custom_loss\n",
    ")\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "model_path = os.path.join(\"PP Models\", f\"{model_version}.keras\")\n",
    "epoch_model_path = os.path.join(\"PP Models (All Epochs)\", f\"{model_version} - \", \"epoch{epoch:02d}.keras\")\n",
    "\n",
    "time_callback = TimeHistory()\n",
    "\n",
    "# Add callbacks for better training\n",
    "callbacks = [\n",
    "    # ValidationMetricCallback(validation_data=(X_val, (y_val, isolated_peaks_val)), metric_name=\"peak_counting_error\"),\n",
    "    # EarlyStopping(monitor=\"peak_counting_error\", patience=patience, restore_best_weights=True, verbose=1),  # Stop if no improvement for {patience} epochs\n",
    "    ModelCheckpoint(model_path, save_best_only=True, monitor=\"val_loss\"),  # Save the best model\n",
    "    ModelCheckpoint(epoch_model_path, save_best_only=False, monitor=\"peak_counting_error\"),  # Save all models model\n",
    "    time_callback\n",
    "]\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))\n",
    "dataset = dataset.shuffle(buffer_size=1000).batch(batch_size)\n",
    "\n",
    "# Validation dataset\n",
    "validation_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=epochs,        # Number of epochs\n",
    "    batch_size=batch_size,  # Batch size\n",
    "    callbacks=callbacks,    # Add callbacks for early stopping and checkpointing\n",
    "    verbose=1               # Verbose output\n",
    ")\n",
    "\n",
    "history.history['epoch_times'] = time_callback.epoch_times\n",
    "\n",
    "with open(os.path.join(\"PP Model Histories\", f\"{model_version}_history.pkl\"), 'wb') as file:\n",
    "    pickle.dump(history.history, file)\n",
    "    \n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Execution time: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST-TRAINING PROCESSING\n",
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate Peak Counting Error for all hyperparameter combos\n",
    "epochs = 25\n",
    "min_epoch = 24\n",
    "threshold_list = [0.01, 0.03, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "lr = 0.001\n",
    "\n",
    "for include_LSTM in [False, True]:\n",
    "    for k in [0, 3]:\n",
    "        for peak_encourage in [15, 5, 10, 1]:\n",
    "            start_time = time.time()\n",
    "            all_epoch_best_thresh = 0\n",
    "            all_epoch_best_error = np.inf\n",
    "            best_epoch = 0\n",
    "\n",
    "            # Preload all models for the current hyperparameter combination\n",
    "            models = {\n",
    "                i: load_model(\n",
    "                    os.path.join(\"PP Models (All Epochs)\", f\"V1_k-{k}_PE-{peak_encourage}_LSTM-{include_LSTM}_Epochs-{epochs}_LR-{lr} - epoch {i:02}.keras\"),\n",
    "                    custom_objects={\"custom_loss\": custom_loss}\n",
    "                )\n",
    "                for i in range(min_epoch, epochs + 1)\n",
    "            }\n",
    "\n",
    "            for i in range(min_epoch, epochs + 1):  # Use epochs instead of hardcoding 25\n",
    "                model_version = f\"V1_k-{k}_PE-{peak_encourage}_LSTM-{include_LSTM}_Epochs-{epochs}_LR-{lr} - epoch {i:02}\"\n",
    "                model = models[i]  # Use preloaded model\n",
    "                test_pred = model.predict(X_val, verbose=0)  # Predicted probabilities\n",
    "                best_error, best_thresh = peak_counting_error(isolated_peaks_val, test_pred, threshold_list=threshold_list)\n",
    "\n",
    "                if best_error < all_epoch_best_error:\n",
    "                    all_epoch_best_error = best_error\n",
    "                    all_epoch_best_thresh = best_thresh\n",
    "                    best_epoch = i\n",
    "\n",
    "            # Load the best model using the preloaded dictionary\n",
    "            best_model = models[best_epoch]\n",
    "            test_pred = best_model.predict(X_test, verbose=0)  # Predicted probabilities\n",
    "            test_error, test_thresh = peak_counting_error(isolated_peaks_test, test_pred, threshold_list=[all_epoch_best_thresh])\n",
    "            assert test_thresh == all_epoch_best_thresh\n",
    "            print(f\"LSTM = {include_LSTM}, k = {k}, PE = {peak_encourage}, Best Val Threshold: {round(all_epoch_best_thresh, 2)}, Best Epoch: {best_epoch}, Test Error: {round(test_error, 5)}\")\n",
    "            elapsed_time = time.time() - start_time\n",
    "            print(f\"Execution time per model: {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File not found: filepath=PP Models (All Epochs)\\V1_k-0_PE-10_LSTM-True_Epochs-25_LR-0.001 - epoch 24.keras. Please ensure the file is an accessible `.keras` zip file.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m model_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPP Models (All Epochs)\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mV1_k-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_PE-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpeak_encourage\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_LSTM-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00minclude_LSTM\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_Epochs-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_LR-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlr\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m - epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_epoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Load model and get predictions\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustom_loss\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mcustom_loss\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m test_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)  \u001b[38;5;66;03m# Predicted probabilities or binary values\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Get spectrum (last 0 is just becuase this has shape (n_Samples, bins, 1))\u001b[39;00m\n",
      "File \u001b[1;32mc:\\ProgramData\\miniconda3\\lib\\site-packages\\keras\\src\\saving\\saving_api.py:200\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, safe_mode)\u001b[0m\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m legacy_h5_format\u001b[38;5;241m.\u001b[39mload_model_from_hdf5(\n\u001b[0;32m    197\u001b[0m         filepath, custom_objects\u001b[38;5;241m=\u001b[39mcustom_objects, \u001b[38;5;28mcompile\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mcompile\u001b[39m\n\u001b[0;32m    198\u001b[0m     )\n\u001b[0;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(filepath)\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 200\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    201\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile not found: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease ensure the file is an accessible `.keras` \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    203\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzip file.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    204\u001b[0m     )\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    206\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    207\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFile format not supported: filepath=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKeras 3 only supports V3 `.keras` files and \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmight have a different name).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    218\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: File not found: filepath=PP Models (All Epochs)\\V1_k-0_PE-10_LSTM-True_Epochs-25_LR-0.001 - epoch 24.keras. Please ensure the file is an accessible `.keras` zip file."
     ]
    }
   ],
   "source": [
    "# CHECKING PREDICTIONS WITH HEAT MAP\n",
    "\n",
    "# Set sample index\n",
    "sample_idx = 55\n",
    "\n",
    "# Set best model parameters\n",
    "best_epoch = 24\n",
    "lr = 0.001\n",
    "epochs = 25\n",
    "include_LSTM = True\n",
    "peak_encourage = 10\n",
    "k=0\n",
    "model_path = os.path.join(\"PP Models (All Epochs)\", f\"V1_k-{k}_PE-{peak_encourage}_LSTM-{include_LSTM}_Epochs-{epochs}_LR-{lr} - epoch {best_epoch:02}.keras\")\n",
    "# Load model and get predictions\n",
    "model = load_model(model_path, custom_objects={\"custom_loss\": custom_loss})\n",
    "test_pred = model.predict(X_test)  # Predicted probabilities or binary values\n",
    "\n",
    "# Get spectrum (last 0 is just becuase this has shape (n_Samples, bins, 1))\n",
    "spectrum = X_val[sample_idx, :, 0]\n",
    "# Get predictions (last 0 is because we want peak predictions)\n",
    "predicted_peaks = test_pred[sample_idx, :, 0] \n",
    "\n",
    "# Frequency axis for the spectrum\n",
    "f = rfftfreq(32768, 1/44100)[0:8192]\n",
    "\n",
    "# Plot the spectrum\n",
    "plt.plot(f/1000, spectrum, label='Synthetic Spectrum')\n",
    "\n",
    "# Add colors depending on percentage of prediction\n",
    "scatter = plt.scatter(f/1000, spectrum, c=predicted_peaks, cmap='viridis', s=15)\n",
    "\n",
    "# Add color bar\n",
    "plt.colorbar(scatter, label='Predicted Peaks Value')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Frequency (kHz)')\n",
    "plt.ylabel('dB SPL (normalized)')\n",
    "plt.title('Spectrum with Scatter Points Colored by Predicted Peaks Value')\n",
    "plt.show()\n",
    "\n",
    "# Here's the filepath I know exists: V1_k-0_PE-10_LSTM-True_Epochs-25_LR-0.001 - epoch 24.keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHECKING PEAK LABELS\n",
    "sample_idx = 0\n",
    "spectrum = X_train[sample_idx, :]\n",
    "peak_labels = y_train[sample_idx, :, 0]\n",
    "\n",
    "# Frequency axis for the spectrum\n",
    "f = rfftfreq(32768, 1/44100)[0:8192]\n",
    "\n",
    "# Find indices where there are peaks (1 in predicted_peaks)\n",
    "peak_indices = np.where(peak_labels==1)[0]\n",
    "\n",
    "# Plot the spectrum\n",
    "plt.plot(f, spectrum, label='Original Spectrum')\n",
    "\n",
    "# Plot the peaks as scatter points\n",
    "plt.scatter(f[peak_indices], spectrum[peak_indices], color='red', label='Labeled Peaks')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('Amplitude')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Custom Loss\n",
    "# Example data: batch_size=4, N=5, nodes=3\n",
    "y_true = np.array([\n",
    "    [[0, 0.5, 0.7], [0, 0.2, -1], [0, 10000, -1000], [0, 0.3, 10], [0, 0.1, 10]],  # Sample 1\n",
    "    [[0, 0.6, 0.3], [1, 0.1, -1], [1, 0.3, 10], [0, 0.4, 10], [0, 0.7, 10]],  # Sample 2\n",
    "    [[0, 0.4, 1.5], [1, 0.8, -1], [1, 0.6, 10], [1, 0.2, 10], [0, 0.9, 10]],  # Sample 3\n",
    "    [[0, 0.5, 0.6], [0, 0.3, -1], [0, 0.7, 10], [1, 0.1, 10], [0, 0.8, 10]],  # Sample 4\n",
    "])\n",
    "\n",
    "y_pred = np.array([\n",
    "    [[0.9, 0.6, 0.8], [0.9, 0.3, 0.5], [0.5, 100000, 1000], [0.7, 0.4, 0.6], [0.2, 0.1, 0.3]],  # Sample 1\n",
    "    [[0.7, 0.5, 0.4], [0.9, 0.2, 0.3], [0.9, 0.4, 0.6], [0.6, 0.7, 0.9], [0.8, 0.7, 0.8]],  # Sample 2\n",
    "    [[0.8, 0.4, 0.5], [0.9, 0.6, 0.8], [0.9, 0.7, 0.5], [0.9, 0.3, 0.6], [0.2, 0.9, 0.7]],  # Sample 3\n",
    "    [[0.9, 0.4, 0.3], [0.9, 0.6, 0.8], [0.6, 0.9, 0.7], [0.9, 0.3, 0.5], [0.8, 0.7, 0.6]],  # Sample 4\n",
    "])\n",
    "\n",
    "# Convert to tensors\n",
    "y_true_tensor = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "y_pred_tensor = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "\n",
    "loss_value = custom_loss(y_true_tensor, y_pred_tensor)\n",
    "print(\"Loss Value:\", loss_value.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a sample\n",
    "i = 1\n",
    "spectrum = X_train[i]\n",
    "f = rfftfreq(32768, 1/44100)[0:8192]\n",
    "\n",
    "labels = y_train[i, :, :]\n",
    "spread_peak_labels = labels[:, 0]\n",
    "indices = np.where(spread_peak_labels == 1)[0]\n",
    "# isolated_peaks = isolated_peaks_train[i]\n",
    "# indices = np.where(isolated_peaks == 1)[0]\n",
    "plt.scatter(f, spectrum, color='blue', s=1)\n",
    "plt.scatter(f[indices], spectrum[indices], color='red', s=1)\n",
    "plt.xlim(3000, 4000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify peak_counting_error\n",
    "isolated_peaks=np.array(\n",
    "    [[0, 1, 0, 0, 1], \n",
    "     [1, 0, 1, 0, 0]]\n",
    "    )\n",
    "predictions=np.array(\n",
    "    [[0.81, 0.91, 0.71, 0.31, 0.91],\n",
    "     [0.81, 0.21, 0.91, 0.96, 0.91]]\n",
    "    )\n",
    "\n",
    "peak_counting_error(isolated_peaks, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get various bin HWHFM of peak labels\n",
    "\n",
    "# Figure out how many bins our peaks are\n",
    "f = rfftfreq(32768, 1/44100)\n",
    "# HWFM (in bins) of a peak with a HWHM of 100Hz\n",
    "bin_width = f[1] - f[0]\n",
    "print(f\"HWFM (in bins) of a peak with a HWHM of 100Hz: {200 / bin_width}\")\n",
    "print(f\"HWFM (in bins) of a peak with a HWHM of 10Hz: {20 / bin_width}\")\n",
    "print(f\"HWFM (in bins) of a peak with a HWHM of 3Hz: {6 / bin_width}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Data extracted and processed from the user input\n",
    "data = [\n",
    "    {\"LSTM\": \"No\", \"k\": 0, \"PE\": 15, \"Best Val Threshold\": 0.9, \"Best Epoch\": 21, \"Test Error\": 5.94415},\n",
    "    {\"LSTM\": \"No\", \"k\": 0, \"PE\": 5, \"Best Val Threshold\": 0.8, \"Best Epoch\": 24, \"Test Error\": 4.74084},\n",
    "    {\"LSTM\": \"No\", \"k\": 0, \"PE\": 10, \"Best Val Threshold\": 0.7, \"Best Epoch\": 24, \"Test Error\": 5.25305},\n",
    "    {\"LSTM\": \"No\", \"k\": 0, \"PE\": 1, \"Best Val Threshold\": 0.4, \"Best Epoch\": 24, \"Test Error\": 5.02094},\n",
    "    {\"LSTM\": \"No\", \"k\": 3, \"PE\": 15, \"Best Val Threshold\": 0.6, \"Best Epoch\": 22, \"Test Error\": 5.75131},\n",
    "    {\"LSTM\": \"No\", \"k\": 3, \"PE\": 5, \"Best Val Threshold\": 0.1, \"Best Epoch\": 25, \"Test Error\": 5.05061},\n",
    "    {\"LSTM\": \"No\", \"k\": 3, \"PE\": 10, \"Best Val Threshold\": 0.3, \"Best Epoch\": 15, \"Test Error\": 5.56457},\n",
    "    {\"LSTM\": \"No\", \"k\": 3, \"PE\": 1, \"Best Val Threshold\": 0.2, \"Best Epoch\": 24, \"Test Error\": 6.48691},\n",
    "    {\"LSTM\": \"Yes\", \"k\": 0, \"PE\": 15, \"Best Val Threshold\": 0.6, \"Best Epoch\": 25, \"Test Error\": 1.9712},\n",
    "    {\"LSTM\": \"Yes\", \"k\": 0, \"PE\": 5, \"Best Val Threshold\": 0.4, \"Best Epoch\": 23, \"Test Error\": 2.5637},\n",
    "    {\"LSTM\": \"Yes\", \"k\": 0, \"PE\": 10, \"Best Val Threshold\": 0.5, \"Best Epoch\": 24, \"Test Error\": 1.85428},\n",
    "    {\"LSTM\": \"Yes\", \"k\": 0, \"PE\": 1, \"Best Val Threshold\": 0.1, \"Best Epoch\": 24, \"Test Error\": 2.68586},\n",
    "    {\"LSTM\": \"Yes\", \"k\": 3, \"PE\": 15, \"Best Val Threshold\": 0.1, \"Best Epoch\": 23, \"Test Error\": 1.95462},\n",
    "    {\"LSTM\": \"Yes\", \"k\": 3, \"PE\": 5, \"Best Val Threshold\": 0.1, \"Best Epoch\": 25, \"Test Error\": 3.19546},\n",
    "    {\"LSTM\": \"Yes\", \"k\": 3, \"PE\": 10, \"Best Val Threshold\": 0.1, \"Best Epoch\": 20, \"Test Error\": 3.64572},\n",
    "    {\"LSTM\": \"Yes\", \"k\": 3, \"PE\": 1, \"Best Val Threshold\": 0.1, \"Best Epoch\": 15, \"Test Error\": 4.69634},\n",
    "]\n",
    "\n",
    "# Create DataFrame and sort by Test Error\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Reorder the columns: Test Error -> Best Val Threshold -> Best Epoch\n",
    "column_order = [\"LSTM\", \"k\", \"PE\", \"Test Error\", \"Best Val Threshold\", \"Best Epoch\"]\n",
    "df_reordered = df[column_order]\n",
    "\n",
    "# Sort by test error\n",
    "df_sorted = df_reordered.sort_values(by=\"Test Error\")\n",
    "\n",
    "# Round the \"Best Val Threshold\" column to 1 decimal place\n",
    "df_sorted[\"Best Val Threshold\"] = df_sorted[\"Best Val Threshold\"].map(\"{:.1f}\".format)\n",
    "df_sorted[\"Test Error\"] = df_sorted[\"Test Error\"].map(\"{:.3f}\".format)\n",
    "\n",
    "# Convert DataFrame to LaTeX table format\n",
    "latex_table = df_sorted.to_latex(index=False, column_format=\"ccc|ccc\", header=[\"LSTM\", \"$k$\", \"$p$\", \"Test Error\", \"Best Val Threshold\", \"Best Epoch\"])\n",
    "\n",
    "# Output LaTeX table\n",
    "print(latex_table)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
