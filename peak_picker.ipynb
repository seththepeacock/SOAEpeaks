{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Conv1D, Dense, Concatenate, TimeDistributed, LSTM, Bidirectional\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from helper_funcs import gen_samples\n",
    "from scipy.fft import rfftfreq\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First navigate to our directory\n",
    "transfer_directory_path = os.path.join(\"Data\", \"synth_transfer_df.parquet\")\n",
    "general_directory_path = os.path.join(\"Data\", \"synth_general_df.parquet\")\n",
    "# Load the dataframes\n",
    "synth_transfer_df = pd.read_parquet(transfer_directory_path)\n",
    "synth_general_df = pd.read_parquet(general_directory_path)\n",
    "# Concatenate (after making sure they share columns) and then reset indices\n",
    "assert list(synth_transfer_df.columns) == list(synth_general_df.columns), \"Column names do not match!\"\n",
    "df = pd.concat([synth_transfer_df, synth_general_df], axis=0)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train (70%) and temp (30%) with stratification\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.3,\n",
    "    stratify=df['species'],  # Stratify based on the 'species' column\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Split temp into test (15%) and validation (15%)\n",
    "test_df, val_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    stratify=temp_df['species'],  # Stratify again to maintain balance\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare samples\n",
    "X_train, y_train, mins_maxes_train, isolated_peaks_train = gen_samples(train_df)\n",
    "X_test, y_test, mins_maxes_test, isolated_peaks_test = gen_samples(test_df)\n",
    "X_val, y_val, mins_maxes_val, isolated_peaks_val = gen_samples(val_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as plt\n",
    "\n",
    "# Check our bin labeling worked\n",
    "spectrum = X_train[0]\n",
    "f = rfftfreq(32768, 1/44100)\n",
    "\n",
    "labels = y_train[0]\n",
    "binary_peak_labels = y_train[:, 0]\n",
    "indices = np.where(binary_peak_labels == 1)[0]\n",
    "plt.plot(f, spectrum)\n",
    "plt.scatter(f[indices], spectrum[indices], color='red')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_func(snr, k=3):\n",
    "    return ((snr/k)**k) / (1 + (snr/k)**k)\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Custom loss function for (batch_size, N, 3):\n",
    "    - Binary cross-entropy for the first output node.\n",
    "    - MSE for the second and third output nodes, masked by the first node's true labels.\n",
    "    - Each bin in each sample is weighted by f(SNR), where SNR is the 3rd node label.\n",
    "    \n",
    "    Args:\n",
    "    y_true: Tensor of true labels, shape (batch_size, N, 3).\n",
    "    y_pred: Tensor of predicted values, shape (batch_size, N, 3).\n",
    "    \n",
    "    Returns:\n",
    "    A scalar tensor representing the combined loss.\n",
    "    \"\"\"\n",
    "\n",
    "    # Mean squared error for the second and third nodes\n",
    "    mse_loss_2 = tf.square(y_true[..., 1] - y_pred[..., 1])\n",
    "    mse_loss_3 = tf.square(y_true[..., 2] - y_pred[..., 2])\n",
    "    mse_loss = mse_loss_2 + mse_loss_3  # Shape (batch_size, N)\n",
    "\n",
    "    # Mask the MSE loss where the first node's true label is 0\n",
    "    mask = tf.cast(y_true[..., 0] > 0, tf.float32)  # Shape (batch_size, N)\n",
    "    masked_mse_loss = mse_loss * mask  # Shape (batch_size, N)\n",
    "    \n",
    "    # Manually calculate binary cross-entropy for the first node\n",
    "    epsilon = 1e-7  # Small constant to prevent log(0)\n",
    "    y_pred_clipped = tf.clip_by_value(y_pred[..., 0], epsilon, 1.0 - epsilon)\n",
    "    bce_loss = -(y_true[..., 0] * tf.math.log(y_pred_clipped) + (1 - y_true[..., 0]) * tf.math.log(1 - y_pred_clipped))  # Shape (batch_size, N)\n",
    "\n",
    "    # Weighting each bin by weight_func(SNR), where SNR is the 3rd node label\n",
    "    snr = y_true[..., 2]  # SNR is the 3rd node label, shape (batch_size, N)\n",
    "    weights = tf.where(snr < 0, tf.ones_like(snr), weight_func(snr))  # If SNR < 0, weight is 1 (fully weight the BCE loss for non-peak bins), else apply weight_func\n",
    "    print(weights)\n",
    "\n",
    "    # Apply weights to the masked MSE loss\n",
    "    weighted_mse_loss = masked_mse_loss * weights  # Shape (batch_size, N)\n",
    "    \n",
    "    # Apply weights to the BCE loss\n",
    "    weighted_bce_loss = bce_loss * weights  # Shape (batch_size, N)\n",
    "\n",
    "    # Average weighted MSE, BCE losses across bins (N) for each sample\n",
    "    mean_mse_loss_per_sample = tf.reduce_mean(weighted_mse_loss, axis=1)  # Mean over N for shape (batch_size,)\n",
    "    mean_bce_loss_per_sample = tf.reduce_mean(weighted_bce_loss, axis=1)\n",
    "\n",
    "    # Combine and average across the batch\n",
    "    total_loss = tf.reduce_mean(mean_bce_loss_per_sample + mean_mse_loss_per_sample)  # Mean over batch size\n",
    "\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "            \n",
    "def peak_counting_error(val_isolated_labels, val_predictions):\n",
    "    M = val_isolated_labels.shape[0] # Number of samples\n",
    "    assert M == val_predictions.shape[0], \"Mismatch in number of samples!\"\n",
    "    best_error = 10000  # Initial large value for minimizing error\n",
    "    best_thresh = None\n",
    "    for thresh in [0.5, 0.6, 0.7, 0.8, 0.9, 0.95]:\n",
    "        current_error = 0\n",
    "        val_predictions_snapped = (val_predictions > thresh).astype(int)\n",
    "        \n",
    "        for row in range(M):\n",
    "            predictions_row = val_predictions_snapped[row, :]\n",
    "            isolated_labels_row = val_isolated_labels[row, :]\n",
    "            \n",
    "            # We want to go along the predictions row, and find continuous chunks of 1s and 0s.\n",
    "            # Every time a chunk ends, we then check how many peaks were truly in that chunk (by counting the 1s in those indices in isolated_labels_row)\n",
    "            # We then add the square of the differences between the predicted number of peaks and the actual number of peaks to total_error\n",
    "            # The predicted number of peaks in a chunk of 1s is always 1, and the predicted number of peaks in a chunk of 0s is always 0.\n",
    "            \n",
    "            # Track the current chunk\n",
    "            current_chunk_value = predictions_row[0]\n",
    "            current_chunk_start = 0\n",
    "\n",
    "            for idx in range(1, len(predictions_row) + 1):  # +1 to handle the last chunk\n",
    "                if idx == len(predictions_row) or predictions_row[idx] != current_chunk_value:\n",
    "                    # Chunk ends here\n",
    "                    chunk_end = idx\n",
    "                    chunk_labels = isolated_labels_row[current_chunk_start:chunk_end]\n",
    "                    \n",
    "                    # Predicted peaks for this chunk\n",
    "                    predicted_peaks = current_chunk_value\n",
    "                    # Actual peaks for this chunk\n",
    "                    actual_peaks = int(chunk_labels.sum())  # Count the 1s in the chunk\n",
    "                    \n",
    "                    # Add squared error to total_error\n",
    "                    current_error += (predicted_peaks - actual_peaks) ** 2\n",
    "                    \n",
    "                    # Start a new chunk\n",
    "                    current_chunk_value = predictions_row[idx] if idx < len(predictions_row) else None\n",
    "                    current_chunk_start = idx\n",
    "        \n",
    "        current_error /= M\n",
    "        print(f\"Peak counting error for threshold {thresh}: {current_error}\")\n",
    "        # Update the best threshold if this one performs better\n",
    "        if current_error < best_error:\n",
    "            best_error = current_error\n",
    "            best_thresh = thresh\n",
    "    \n",
    "    print(f\"Best threshold: {best_thresh}, Best Peak Counting Error: {best_error}\")\n",
    "    return best_error\n",
    "        \n",
    "\n",
    "\n",
    "class ValidationMetricCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, validation_data, metric_name=\"val_custom_mae\"):\n",
    "        super().__init__()\n",
    "        self.validation_data = validation_data\n",
    "        self.metric_name = metric_name\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        val_x, (val_y, val_isolated_labels) = self.validation_data  # Unpack extra labels\n",
    "        val_predictions = self.model.predict(val_x, verbose=0)\n",
    "        \n",
    "        # Compute your custom metric (e.g., Mean Absolute Error)\n",
    "        val_metric = peak_counting_error(val_isolated_labels, val_predictions)\n",
    "\n",
    "        # Add the validation metric to logs\n",
    "        logs[self.metric_name] = val_metric.numpy()\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}: {self.metric_name} = {val_metric.numpy()}\")\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Figure out how many bins our peaks are\n",
    "f = rfftfreq(32768, 1/44100)\n",
    "# HWFM (in bins) of a peak with a HWHM of 100Hz\n",
    "bin_width = f[1] - f[0]\n",
    "print(f\"HWFM (in bins) of a peak with a HWHM of 100Hz: {200 / bin_width}\")\n",
    "print(f\"HWFM (in bins) of a peak with a HWHM of 10Hz: {20 / bin_width}\")\n",
    "print(f\"HWFM (in bins) of a peak with a HWHM of 3Hz: {6 / bin_width}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define name for this model\n",
    "model_version = \"PP V1\"\n",
    "\n",
    "# Define the input length / number of frequency bins (N)\n",
    "N = 8192\n",
    "\n",
    "# Input layer\n",
    "input_layer = Input(shape=(N, 1), name=\"Input\")\n",
    "\n",
    "# Inception-like layer with 1D convolutions\n",
    "convs = []\n",
    "# We'll base our kernel choices on the hwhm distribution of the peaks. \n",
    "# Thin peaks are in 3Hz-10Hz range --> 5-15 bins\n",
    "# Wide peaks are in 10Hz-100Hz range --> 15-149 bins\n",
    "# We choose filters at a range of scales, odd (to facilitate being cenetered around a peak)\n",
    "# and we want more filters for the medium-small range since there are more peaks at this scale.\n",
    "# Otherwise largely arbitrarily.\n",
    "kernels = [(3, 4), (5, 8), (9, 16), (15, 32), (31, 32), (55, 32), (71, 16), (101, 8), (149, 4), (201, 2)]\n",
    "for kernel_size, num_filters in kernels:\n",
    "    convs.append(Conv1D(num_filters, kernel_size=kernel_size, activation='relu', padding='same', name=f\"Conv_{kernel_size}\")(input_layer))\n",
    "\n",
    "# Concatenate the outputs of all convolutional layers\n",
    "concat_layer = Concatenate(name=\"Inception_Concat\")(convs)\n",
    "\n",
    "# Time Distributed Dense Layers\n",
    "td_dense64 = TimeDistributed(Dense(64, activation='relu'), name=\"Dense_64\")(concat_layer)\n",
    "td_dense32A = TimeDistributed(Dense(32, activation='relu'), name=\"Dense_32A\")(td_dense64)\n",
    "# bd_LSTM = Bidirectional(LSTM(16, return_sequences=True), name=\"LSTM\")(td_dense32A)\n",
    "# td_dense32B = TimeDistributed(Dense(32, activation='relu'), name=\"Dense_32B\")(bd_LSTM)\n",
    "td_dense32B = TimeDistributed(Dense(32, activation='relu'), name=\"Dense_32B\")(td_dense32A)\n",
    "td_dense16 = TimeDistributed(Dense(16, activation='relu'), name=\"Dense_16\")(td_dense32B)\n",
    "\n",
    "# Final layer with 3 outputs per input bin\n",
    "output = TimeDistributed(Dense(3, activation=None), name=\"Output\")(td_dense16)\n",
    "\n",
    "# Define the model\n",
    "model = Model(inputs=input_layer, outputs=output, name=model_version)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=custom_loss, metrics=[\"custom_accuracy\"])\n",
    "\n",
    "# Summary\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define batch size, number of epochs, and patience\n",
    "batch_size = 32\n",
    "epochs = 15\n",
    "patience = 3\n",
    "\n",
    "weight_path=os.path.join(\"PP Weights\", f\"{model_version}.h5\")\n",
    "\n",
    "\n",
    "# Add callbacks for better training\n",
    "callbacks = [\n",
    "    ValidationMetricCallback(validation_data=(X_val, (y_val, isolated_peaks_val)), metric_name=\"val_custom_accuracy\"),\n",
    "    EarlyStopping(monitor=\"val_custom_accuracy\", patience=patience, restore_best_weights=True, verbose=1),  # Stop if no improvement for 5 epochs\n",
    "    ModelCheckpoint(weight_path, save_best_only=True, monitor='val_loss')  # Save the best model\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train,                # Training data\n",
    "    y_train,                # Training labels\n",
    "    validation_data=(X_val, y_val),  # Validation data\n",
    "    epochs=epochs,        # Number of epochs\n",
    "    batch_size=batch_size,  # Batch size\n",
    "    callbacks=callbacks,    # Add callbacks for early stopping and checkpointing\n",
    "    verbose=1               # Verbose output\n",
    ")\n",
    "\n",
    "with open(f'{model_version}_history.pkl', 'wb') as file:\n",
    "    pickle.dump(history.history, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "test_loss = model.evaluate(X_test, y_test, verbose=1)  # Verbose output for evaluation\n",
    "\n",
    "print(\"Test Loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify Custom Loss\n",
    "# Example data: batch_size=4, N=5, nodes=3\n",
    "y_true = np.array([\n",
    "    [[0, 0.5, 0.7], [0, 0.2, -1], [0, 10000, -1000], [0, 0.3, 10], [0, 0.1, 10]],  # Sample 1\n",
    "    [[0, 0.6, 0.3], [1, 0.1, -1], [1, 0.3, 10], [0, 0.4, 10], [0, 0.7, 10]],  # Sample 2\n",
    "    [[0, 0.4, 1.5], [1, 0.8, -1], [1, 0.6, 10], [1, 0.2, 10], [0, 0.9, 10]],  # Sample 3\n",
    "    [[0, 0.5, 0.6], [0, 0.3, -1], [0, 0.7, 10], [1, 0.1, 10], [0, 0.8, 10]],  # Sample 4\n",
    "])\n",
    "\n",
    "y_pred = np.array([\n",
    "    [[0.9, 0.6, 0.8], [0.9, 0.3, 0.5], [0.5, 100000, 1000], [0.7, 0.4, 0.6], [0.2, 0.1, 0.3]],  # Sample 1\n",
    "    [[0.7, 0.5, 0.4], [0.9, 0.2, 0.3], [0.9, 0.4, 0.6], [0.6, 0.7, 0.9], [0.8, 0.7, 0.8]],  # Sample 2\n",
    "    [[0.8, 0.4, 0.5], [0.9, 0.6, 0.8], [0.9, 0.7, 0.5], [0.9, 0.3, 0.6], [0.2, 0.9, 0.7]],  # Sample 3\n",
    "    [[0.9, 0.4, 0.3], [0.9, 0.6, 0.8], [0.6, 0.9, 0.7], [0.9, 0.3, 0.5], [0.8, 0.7, 0.6]],  # Sample 4\n",
    "])\n",
    "\n",
    "# Convert to tensors\n",
    "y_true_tensor = tf.convert_to_tensor(y_true, dtype=tf.float32)\n",
    "y_pred_tensor = tf.convert_to_tensor(y_pred, dtype=tf.float32)\n",
    "\n",
    "loss_value = custom_loss(y_true_tensor, y_pred_tensor)\n",
    "print(\"Loss Value:\", loss_value.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify peak_counting_error\n",
    "val_isolated_labels=np.array(\n",
    "    [[0, 1, 0, 0, 1], \n",
    "     [1, 0, 1, 0, 0]]\n",
    "    )\n",
    "val_predictions=np.array(\n",
    "    [[0.81, 0.91, 0.71, 0.31, 0.91],\n",
    "     [0.81, 0.21, 0.91, 0.96, 0.91]]\n",
    "    )\n",
    "\n",
    "peak_counting_error(val_isolated_labels, val_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
