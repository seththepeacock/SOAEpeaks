{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from plots import *\n",
    "import scipy.io\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def plot_mags(path, t_win=1, sf=False):\n",
    "    wf = np.loadtxt(path)\n",
    "    m = get_mags(wf, sr=44100, t_win=t_win, dict=True)\n",
    "    mags = m['mags']\n",
    "    freq_ax = m['freq_ax']\n",
    "    plt.plot(freq_ax, np.log10(mags)*10)\n",
    "    plt.title(str(path).split(\"\\\\\")[-1])\n",
    "    if sf:\n",
    "        plt.savefig(str(path).split(\"\\\\\")[-1].split(\".\")[0] + \".png\")\n",
    "    plt.show()\n",
    "def plot_supp(path, sf=False):\n",
    "    data = np.loadtxt(path)\n",
    "    freqs = data[:, 0]\n",
    "    mags = data[:, 1]\n",
    "    plt.plot(freqs, mags)\n",
    "    plt.title(str(path).split(\"\\\\\")[-1])\n",
    "    if sf:\n",
    "        plt.savefig(str(path).split(\"\\\\\")[-1].split(\".\")[0] + \".png\")\n",
    "    plt.show()\n",
    "\n",
    "def has_supptone(mags):\n",
    "    # find cutoff freq, check if anything above this has > 20 dB\n",
    "    cutoff = mags[0]\n",
    "    return True\n",
    "    \n",
    "    \n",
    "# get the main directory in my computer\n",
    "main_path_str = \"C:\\\\Users\\\\Owner\\OneDrive\\\\Desktop\\\\SOAE Data\\\\\"\n",
    "# we'll process each subfolder separately since each is likely to have its own quirks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path1 = \"C:\\\\Users\\\\Owner\\OneDrive\\\\Desktop\\\\SOAE Data\\\\Pre-2014 Data\\\\Geckos et al MIT\\\\\"\n",
    "# path2 = \"01.26.05\\\\\"\n",
    "# path1 = r\"C:\\Users\\Owner\\OneDrive\\Desktop\\SOAE Data\\Pre-2014 Data\\Human (UofA S&A via Wiggio)\"\n",
    "# path2 = \"\\\\07.02.09\"\n",
    "path1 = r\"C:\\Users\\Owner\\OneDrive\\Desktop\\SOAE Data\\Pre-2014 Data\\Lizards CUMC2011\"\n",
    "path2 = r\"\\05.16.11\"\n",
    "# path1 = r\"C:\\Users\\Owner\\OneDrive\\Desktop\\SOAE Data\\York Data\"\n",
    "# path2 = r\"\\04.12.17\"\n",
    "sf = False\n",
    "for fp in Path(path1+path2).rglob('*'):\n",
    "    fn = fp.name\n",
    "    ext = fp.suffix\n",
    "    if \"README\" in fn or ext in ('.rtf' '.pdf'):\n",
    "        print(f\"Skipping {fn}\")\n",
    "        continue\n",
    "    try:\n",
    "        plot_supp(fp, sf=sf)\n",
    "    except:\n",
    "        print(\"Waveform detecting, calculating mags\")\n",
    "        plot_mags(fp, t_win=1, sf=sf)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/4247\n",
      "Processing file 2/4247\n",
      "Processing file 3/4247\n",
      "Processing file 4/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.25.12\\MP1learSOAEsupp1.txt -- true suppression tone!\n",
      "Processing file 5/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.25.12\\MP1learSOAEsupp2.txt -- true suppression tone!\n",
      "Processing file 6/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.25.12test\\CBrearSOAEsupp1.txt -- true suppression tone!\n",
      "Processing file 7/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.25.12testB\\AOrearSOAEsupp1.txt -- true suppression tone!\n",
      "Processing file 8/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.26.12\\MP2learSOAEsupp1.txt -- true suppression tone!\n",
      "Processing file 9/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.26.12\\MP2learSOAEsupp2.txt -- true suppression tone!\n",
      "Processing file 10/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.26.12\\MP3learSOAEsupp1.txt -- true suppression tone!\n",
      "Processing file 11/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.26.12\\MP3learSOAEsupp2.txt -- true suppression tone!\n",
      "Processing file 12/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.26.12\\MP6rearSOAEsupp1.txt -- true suppression tone!\n",
      "Processing file 13/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.27.12\\MP10learSOAEsupp1.txt -- true suppression tone!\n",
      "Processing file 14/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.27.12\\MP7learSOAEsupp1.txt -- true suppression tone!\n",
      "Processing file 15/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.28.12\\MP13learSOAEsupp1.txt -- true suppression tone!\n",
      "Processing file 16/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.28.12\\MP16learSOAEsupp1.txt -- true suppression tone!\n",
      "Processing file 17/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.28.12\\MP16learSOAEsupp2.txt -- true suppression tone!\n",
      "Processing file 18/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.28.12\\MP9learSOAEsupp1.txt -- true suppression tone!\n",
      "Processing file 19/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.28.12\\MP9learSOAEsupp2.txt -- true suppression tone!\n",
      "Processing file 20/4247\n",
      "Processing file 21/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.29.12\\MP19learSOAEsupp1.txt -- true suppression tone!\n",
      "Processing file 22/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.29.12\\MP19learSOAEsupp2.txt -- true suppression tone!\n",
      "Processing file 23/4247\n",
      "Processing file 24/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.29.12\\MP20learSOAEsupp1.txt -- true suppression tone!\n",
      "Processing file 25/4247\n",
      "Skipping Pre-2014 Data\\Ferrets (Nottingham 2012)\\06.29.12\\MP21learSOAEsupp1.txt -- true suppression tone!\n",
      "Processing file 26/4247\n",
      "Processing file 27/4247\n",
      "Processing file 28/4247\n",
      "Processing file 29/4247\n",
      "Processing file 30/4247\n",
      "Processing file 31/4247\n",
      "Processing file 32/4247\n",
      "Processing file 33/4247\n",
      "Processing file 34/4247\n",
      "Processing file 35/4247\n",
      "Processing file 36/4247\n",
      "Processing file 37/4247\n",
      "Processing file 38/4247\n",
      "Processing file 39/4247\n",
      "Processing file 40/4247\n",
      "Processing file 41/4247\n",
      "Processing file 42/4247\n",
      "Processing file 43/4247\n",
      "Processing file 44/4247\n",
      "Processing file 45/4247\n",
      "Processing file 46/4247\n",
      "Processing file 47/4247\n",
      "Processing file 48/4247\n",
      "Processing file 49/4247\n",
      "Processing file 50/4247\n",
      "Processing file 51/4247\n",
      "Processing file 52/4247\n",
      "Processing file 53/4247\n",
      "Skipping Pre-2014 Data\\Geckos et al MIT\\01.05.07\\PNKrearSOAEsupp1.txt -- true suppression tone!\n",
      "Processing file 54/4247\n",
      "Skipping Pre-2014 Data\\Geckos et al MIT\\01.05.07\\PNKrearSOAEsupp2.txt -- true suppression tone!\n",
      "Processing file 55/4247\n",
      "Skipping Pre-2014 Data\\Geckos et al MIT\\01.05.07\\PNKrearSOAEsupp3.txt -- true suppression tone!\n",
      "Processing file 56/4247\n",
      "Skipping Pre-2014 Data\\Geckos et al MIT\\01.05.07\\PNKrearSOAEsupp4.txt -- true suppression tone!\n",
      "Processing file 57/4247\n",
      "Skipping Pre-2014 Data\\Geckos et al MIT\\01.05.07\\PNKrearSOAEsupp5.txt -- true suppression tone!\n",
      "Processing file 58/4247\n",
      "Skipping Pre-2014 Data\\Geckos et al MIT\\01.05.07\\PNKrearSOAEsupp6.txt -- true suppression tone!\n",
      "Processing file 59/4247\n",
      "Processing file 60/4247\n",
      "Skipping Pre-2014 Data\\Geckos et al MIT\\01.07.06\\CBrearSOAE1.txt -- Chris can't say if good or not\n",
      "Processing file 61/4247\n",
      "Skipping Pre-2014 Data\\Geckos et al MIT\\01.15.05\\KMlearSOAE1.txt -- Chris can't say if good or not\n",
      "Processing file 62/4247\n",
      "Skipping Pre-2014 Data\\Geckos et al MIT\\01.15.05\\KMlearSOAE2.txt -- Chris can't say if good or not\n",
      "Processing file 63/4247\n",
      "Skipping Pre-2014 Data\\Geckos et al MIT\\01.15.05\\KMrearSOAE1.txt -- Chris can't say if good or not\n",
      "Processing file 64/4247\n",
      "Skipping Pre-2014 Data\\Geckos et al MIT\\01.15.05\\KMrearSOAE2.txt -- Chris can't say if good or not\n",
      "Processing file 65/4247\n",
      "Skipping Pre-2014 Data\\Geckos et al MIT\\01.15.05\\KMrearSOAE3.txt -- Chris can't say if good or not\n",
      "Processing file 66/4247\n",
      "Skipping Pre-2014 Data\\Geckos et al MIT\\01.18.05\\TlearSOAE1.txt -- Chris can't say if good or not\n",
      "Processing file 67/4247\n",
      "Processing file 68/4247\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "UH OH Pre-2014 Data\\Geckos et al MIT\\01.26.05\\CrearBT2soae1.txt didn't fall into any categories:",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 102\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUh oh! Issue when loading \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 102\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUH OH \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfps\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m didn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt fall into any categories:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# Get species\u001b[39;00m\n\u001b[0;32m    110\u001b[0m subfolder_species \u001b[38;5;241m=\u001b[39m subfolder\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mRuntimeError\u001b[0m: UH OH Pre-2014 Data\\Geckos et al MIT\\01.26.05\\CrearBT2soae1.txt didn't fall into any categories:"
     ]
    }
   ],
   "source": [
    "folder = \"Pre-2014 Data\"\n",
    "    \n",
    "# We'll build our dataframe by making a dictionary of lists and appending to them\n",
    "data = {\n",
    "    'filepath': [],\n",
    "    'data': [],  \n",
    "    'species': [],\n",
    "    'sr': [],\n",
    "}\n",
    "\n",
    "# First navigate to our directory\n",
    "directory_path = Path(main_path_str + folder)\n",
    "\n",
    "# track which file we're on\n",
    "n_files = sum(1 for _ in directory_path.rglob('*') if _.is_file())\n",
    "n_current=0\n",
    "n_readme = 0\n",
    "n_tube = 0\n",
    "n_earsoae = 0\n",
    "n_supptone = 0\n",
    "n_suppgood = 0\n",
    "n_wf = 0\n",
    "# now loop through all files in that collection\n",
    "for fp in directory_path.rglob('*'):\n",
    "    # Make sure it's a file\n",
    "    if fp.is_file() == False:  \n",
    "        continue\n",
    "    \n",
    "    # track which file we're on\n",
    "    n_current += 1\n",
    "    print(f\"Processing file {n_current}/{n_files}\")\n",
    "    # Get various versions of the filepath/filename\n",
    "    # Cut off the beginning of the filepath since it's unnecessary for our dataframe (fps = file path shortened)\n",
    "    main_path = Path(main_path_str)\n",
    "    fps = str(fp.relative_to(main_path))\n",
    "    # Also get subfolder (if applicable)\n",
    "    if len(fps.split(\"\\\\\")) > 1:\n",
    "        subfolder = fps.split(\"\\\\\")[1]\n",
    "    else:\n",
    "        subfolder = \"NA\"\n",
    "    # Get the filename itself (without its containing folders), extension, and uppercase version\n",
    "    fn = fp.name\n",
    "    ext = fp.suffix\n",
    "    fnU = fn.upper()\n",
    "    \n",
    "    if \"README\" in fnU or ext not in ('.txt' '.mat'):\n",
    "        # print(f\"Skipping {fps} -- README or wrong extension\")\n",
    "        n_readme += 1\n",
    "        continue\n",
    "    \n",
    "    elif \"TUBE\" in fnU:\n",
    "        # print(f\"Skipping {fps} -- Tube file\")\n",
    "        n_tube += 1\n",
    "        continue\n",
    "    \n",
    "    elif \"SUPP\" in fnU:\n",
    "        if ext == '.txt':\n",
    "            mags = np.loadtxt(fp)\n",
    "        try: \n",
    "            if mags.shape[1] != 2:\n",
    "                raise RuntimeError(f\"Supp file from {fps} isn't two columns!\")\n",
    "        except:\n",
    "            raise(f\"Supp file from {fps} isn't 2D!\")\n",
    "        if has_supptone(mags):\n",
    "            print(f\"Skipping {fps} -- true suppression tone!\")\n",
    "            n_supptone += 1\n",
    "            if \"NOSUPP\" in fnU:\n",
    "                raise RuntimeError(f\"Our suppression tone detector is wrong! {fps} shouldn't have a suppression tone...\")\n",
    "        else:\n",
    "            # add a samplerate of 0 to indicate this has been pre-fft'd\n",
    "            data['sr'].append(0)\n",
    "            data['data'].append(mags)\n",
    "            n_suppgood += 1\n",
    "    \n",
    "    elif (\"EAR\" in fnU and \"SOAE\" in fnU) and (\"WF\" not in fnU and \"WAVEFORM\" not in fnU and \"SUPP\" not in fnU):\n",
    "        print(f\"Skipping {fps} -- Chris can't say if good or not\")\n",
    "        n_readme += 1\n",
    "    \n",
    "    elif \"WF\" in fnU or \"WAVEFORM\" in fnU:\n",
    "    # we must have a waveform if we got here\n",
    "        # Check if it's a .txt or .mat file\n",
    "        try:\n",
    "            if ext == '.mat':\n",
    "                mat = scipy.io.loadmat(fp)\n",
    "                if 'wf' in mat:\n",
    "                    wf = np.squeeze(mat['wf'])\n",
    "                else: \n",
    "                    print(f\"Not sure how to process {fps}\")\n",
    "            if ext == '.txt':\n",
    "                wf = np.loadtxt(fp)\n",
    "            # Let's make sure this waveform is a 1D array\n",
    "            if len(wf.shape) > 1:\n",
    "                raise RuntimeError(f\"Waveform from {fps} isn't 1D!\")\n",
    "            \n",
    "            # add to the dataframe \n",
    "            data['sr'].append(44100)\n",
    "            data['data'].append(wf)\n",
    "            n_wf += 1\n",
    "        except:\n",
    "            f\"Uh oh! Issue when loading {fps}\"\n",
    "    else:\n",
    "        raise RuntimeError(f\"UH OH {fps} didn't fall into any categories:\")\n",
    "        \n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "            \n",
    "    # Get species\n",
    "    subfolder_species = subfolder.split(\" \")[0]\n",
    "    match subfolder_species:\n",
    "        case 'Geckos' | 'Lizards':\n",
    "            species = 'Lizard'\n",
    "        case 'Tigers':\n",
    "            species = 'Tiger'\n",
    "        case _:\n",
    "            species = subfolder_species\n",
    "\n",
    "    data['filepath'].append(fps)\n",
    "    data['species'].append(species)\n",
    "\n",
    "# turn this into a pandas dataframe\n",
    "df = pd.DataFrame(data)\n",
    "# save this as a parquet file for efficient dataframe storage (use pyarrow since the 'wf' column has different length lists)\n",
    "df.to_parquet(f'{folder}.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file 1/26\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 87\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# add everything to our df dict\u001b[39;00m\n\u001b[0;32m     86\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilepath\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(fps)\n\u001b[1;32m---> 87\u001b[0m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mappend(wf)\n\u001b[0;32m     88\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecies\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(species)\n\u001b[0;32m     89\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msr\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(sr)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"Extra Owl\"\n",
    "    \n",
    "# We'll build our dataframe by making a dictionary of lists and appending to them\n",
    "data = {\n",
    "    'filepath': [],\n",
    "    'wf': [],  \n",
    "    'species': [],\n",
    "    'sr': [],\n",
    "}\n",
    "\n",
    "# First navigate to our directory\n",
    "directory_path = Path(main_path_str + folder)\n",
    "\n",
    "# track which file we're on\n",
    "n_files = sum(1 for _ in directory_path.rglob('*') if _.is_file())\n",
    "n_current=0\n",
    "\n",
    "# now loop through all files in that collection\n",
    "for fp in directory_path.rglob('*'):\n",
    "    # Check if it's a file\n",
    "    if fp.is_file():  \n",
    "        n_current += 1\n",
    "        print(f\"Processing file {n_current}/{n_files}\")\n",
    "        # Cut off the beginning of the filepath since it's unnecessary for our dataframe (fps = file path shortened)\n",
    "        main_path = Path(main_path_str)\n",
    "        fps = str(fp.relative_to(main_path))\n",
    "        \n",
    "        # Get the filename itself (without its containing folders)\n",
    "        fn = fp.name\n",
    "        # Also uppercase\n",
    "        fnU = fn.upper()\n",
    "        \n",
    "        # now we actually open the waveform here\n",
    "        # Check if it's a .txt or .mat file\n",
    "        try:\n",
    "            if fp.suffix == '.mat':\n",
    "                mat = scipy.io.loadmat(fp)\n",
    "                if 'wf' in mat:\n",
    "                    wf = np.squeeze(mat['wf'])\n",
    "                else: \n",
    "                    print(f\"Not sure how to process {fp}\")\n",
    "            if fp.suffix == '.txt':\n",
    "                wf = np.loadtxt(fp)\n",
    "            # Let's make sure this waveform is a 1D array\n",
    "            if len(wf.shape) > 1:\n",
    "                print(f\"Waveform from {fps} isn't 1D!\")\n",
    "        except:\n",
    "            f\"Uh oh! Issue when loading {fp}\"\n",
    "            \n",
    "        if str(fps).split(\"\\\\\")[1]=='Oldenberg Data (2013) (44.1kHz)':\n",
    "            sr = 44100\n",
    "            species = \"Owl\"\n",
    "        elif str(fps).split(\"\\\\\")[1]=='Pim owl files (48 kHz)':\n",
    "            sr = 48000\n",
    "            species = \"Owl\"\n",
    "        else:\n",
    "            print(\"UH OH WHERE ARE WE\")\n",
    "        \n",
    "            \n",
    "                \n",
    "        # add everything to our df dict\n",
    "        data['filepath'].append(fps)\n",
    "        data['data'].append(wf)\n",
    "        data['species'].append(species)\n",
    "        data['sr'].append(sr)\n",
    "\n",
    "# turn this into a pandas dataframe\n",
    "df = pd.DataFrame(data)\n",
    "# save this as a parquet file for efficient dataframe storage (use pyarrow since the 'wf' column has different length lists)\n",
    "df.to_parquet(f'{folder}.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = \"Lots of Data\"\n",
    "    \n",
    "# We'll build our dataframe by making a dictionary of lists and appending to them\n",
    "data = {\n",
    "    'filepath': [],\n",
    "    'wf': [],  \n",
    "    'species': [],\n",
    "    'sr': [],\n",
    "}\n",
    "\n",
    "# First navigate to our directory\n",
    "directory_path = Path(main_path_str + folder)\n",
    "\n",
    "# track which file we're on\n",
    "n_files = sum(1 for _ in directory_path.rglob('*') if _.is_file())\n",
    "n_current=0\n",
    "\n",
    "# now loop through all files in that collection\n",
    "for fp in directory_path.rglob('*'):\n",
    "    # Check if it's a file\n",
    "    if fp.is_file():  \n",
    "        n_current += 1\n",
    "        print(f\"Processing file {n_current}/{n_files}\")\n",
    "        # Cut off the beginning of the filepath since it's unnecessary for our dataframe (fps = file path shortened)\n",
    "        main_path = Path(main_path_str)\n",
    "        fps = str(fp.relative_to(main_path))\n",
    "        \n",
    "        # Also get ubfolder (if applicable)\n",
    "        if len(fps.split(\"\\\\\")) > 1:\n",
    "            subfolder = fps.split(\"\\\\\")[1]\n",
    "        else:\n",
    "            subfolder = \"NA\"\n",
    "        \n",
    "        # Get the filename itself (without its containing folders)\n",
    "        fn = fp.name\n",
    "        # now we actually open the waveform here\n",
    "        # Check if it's a .txt or .mat file\n",
    "        try:\n",
    "            if fp.suffix == '.mat':\n",
    "                mat = scipy.io.loadmat(fp)\n",
    "                if 'wf' in mat:\n",
    "                    wf = np.squeeze(mat['wf'])\n",
    "                else: \n",
    "                    print(f\"Not sure how to process {fp}\")\n",
    "            if fp.suffix == '.txt':\n",
    "                wf = np.loadtxt(fp)\n",
    "            # Let's make sure this waveform is a 1D array\n",
    "            if len(wf.shape) > 1:\n",
    "                print(f\"Waveform from {fps} isn't 1D!\")\n",
    "        except:\n",
    "            f\"Uh oh! Issue when loading {fp}\"\n",
    "            \n",
    "        # Get species\n",
    "        subfolder_species = subfolder.split(\".\")[3]\n",
    "        \n",
    "        match subfolder_species:\n",
    "            case 'tokay':\n",
    "                species = \"Tokay\"\n",
    "            case 'tegu':\n",
    "                species = \"Tegu\"\n",
    "            case 'human':\n",
    "                species = \"Human\"\n",
    "            case 'skink':\n",
    "                species = \"Skink\"\n",
    "            case 'owl':\n",
    "                species = \"Owl\"\n",
    "            case 'anolis':\n",
    "                species = \"Anolis\"\n",
    "            case 'ACsb42':\n",
    "                species = \"Anolis\"\n",
    "            case _:\n",
    "                print(f\"Couldn't find the species of {fn}\")\n",
    "        \n",
    "        # These all should have the standard sample rate\n",
    "        sr = 44100\n",
    "                \n",
    "        # add everything to our df dict\n",
    "        data['filepath'].append(fps)\n",
    "        data['data'].append(wf)\n",
    "        data['species'].append(species)\n",
    "        data['sr'].append(sr)\n",
    "\n",
    "# turn this into a pandas dataframe\n",
    "df = pd.DataFrame(data)\n",
    "# save this as a parquet file for efficient dataframe storage (use pyarrow since the 'wf' column has different length lists)\n",
    "df.to_parquet(f'{folder}.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
